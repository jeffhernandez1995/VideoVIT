{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6942142-cdf4-44c8-872a-528e1df07d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"]=\"2\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import matplotlib.gridspec as gridspec\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38694e4c-0107-4de9-8232-1d997593d6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seeds for reproducibility.\n",
    "SEED = 42\n",
    "keras.utils.set_random_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# DATA\n",
    "BUFFER_SIZE = 300\n",
    "BATCH_SIZE = 64\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (15, 120, 160, 3)\n",
    "TIME_LEN = INPUT_SHAPE[0]\n",
    "OUTPUT_SHAPE = (120, 160, 3)\n",
    "NUM_CLASSES = 6\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 5e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# PRETRAINING\n",
    "EPOCHS = 500\n",
    "\n",
    "# AUGMENTATION\n",
    "IMAGE_SIZE = 48  # We will resize input images to this size.\n",
    "PATCH_SIZE = 6  # Size of the patches to be extracted from the input images.\n",
    "CROP_SIZE = 100\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "MASK_PROPORTION = 0.75  # We have found 75% masking to give us the best results.\n",
    "\n",
    "# ENCODER and DECODER\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "ENC_PROJECTION_DIM = 128\n",
    "DEC_PROJECTION_DIM = 64\n",
    "ENC_NUM_HEADS = 4\n",
    "ENC_LAYERS = 8\n",
    "DEC_NUM_HEADS = 4\n",
    "DEC_LAYERS = (\n",
    "    4  # The decoder is lightweight but should be reasonably deep for reconstruction.\n",
    ")\n",
    "ENC_TRANSFORMER_UNITS = [\n",
    "    ENC_PROJECTION_DIM * 2,\n",
    "    ENC_PROJECTION_DIM,\n",
    "]  # Size of the transformer layers.\n",
    "DEC_TRANSFORMER_UNITS = [\n",
    "    DEC_PROJECTION_DIM * 2,\n",
    "    DEC_PROJECTION_DIM,\n",
    "]\n",
    "\n",
    "\n",
    "MIXED_PRECISION = False\n",
    "XLA_ACCELERATE = False\n",
    "\n",
    "if MIXED_PRECISION:\n",
    "    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
    "    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "    mixed_precision.set_policy(policy)\n",
    "    print('Mixed precision enabled')\n",
    "\n",
    "if XLA_ACCELERATE:\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    print('Accelerated Linear Algebra enabled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac1b665-3378-4d69-8cf6-bbebcbecd779",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Returns a Dataset for reading from a SageMaker PipeMode channel.\"\"\"\n",
    "features = {\n",
    "    'video': tf.io.FixedLenFeature([], tf.string),\n",
    "    'frame': tf.io.FixedLenFeature([], tf.string),\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "def parse(record):\n",
    "\n",
    "    parsed = tf.io.parse_single_example(\n",
    "        serialized=record,\n",
    "        features=features\n",
    "    )\n",
    "    video_raw = parsed['video']\n",
    "    video_raw = tf.io.decode_raw(video_raw, tf.uint8)\n",
    "    video_raw = tf.cast(video_raw, tf.float32)\n",
    "\n",
    "    label = parsed['label']\n",
    "    label = tf.cast(label, tf.float32)\n",
    "\n",
    "    video_raw = tf.reshape(video_raw, INPUT_SHAPE[:3] + (1,))\n",
    "    video_raw = tf.concat([video_raw, video_raw, video_raw], axis=-1)\n",
    "\n",
    "    return video_raw, label\n",
    "\n",
    "\n",
    "def left_right_flip(video):\n",
    "    '''\n",
    "    Performs tf.image.flip_left_right on entire list of video frames.\n",
    "    Work around since the random selection must be consistent for entire video\n",
    "    :param video: Tensor constaining video frames (N,H,W,3)\n",
    "    :return: video: Tensor constaining video frames left-right flipped (N,H,W,3)\n",
    "    '''\n",
    "    video_list = tf.unstack(video, axis=1)\n",
    "    for i in range(len(video_list)):\n",
    "        video_list[i] = tf.image.flip_left_right(video_list[i])\n",
    "    video = tf.stack(video_list, axis=1)\n",
    "    return video\n",
    "\n",
    "\n",
    "def random_crop(video, size):\n",
    "    # (T, H, W, 3)\n",
    "    shape = tf.shape(video)\n",
    "    size = tf.convert_to_tensor(size, dtype=shape.dtype)\n",
    "    h_diff = shape[2] - size[1]\n",
    "    w_diff = shape[3] - size[0]\n",
    "\n",
    "    dtype = shape.dtype\n",
    "    rands = tf.random.uniform(shape=[2], minval=0, maxval=dtype.max, dtype=dtype)\n",
    "    h_start = tf.cast(rands[0] % (h_diff + 1), dtype)\n",
    "    w_start = tf.cast(rands[1] % (w_diff + 1), dtype)\n",
    "    size = tf.cast(size, tf.int32)\n",
    "    video_list = tf.unstack(video, axis=1)\n",
    "    for i in range(len(video_list)):\n",
    "        video_list[i] = tf.image.crop_to_bounding_box(\n",
    "            video_list[i],\n",
    "            h_start, w_start,\n",
    "            size[1], size[0]\n",
    "        )\n",
    "    video = tf.stack(video_list, axis=1)\n",
    "\n",
    "    return video\n",
    "\n",
    "\n",
    "def resize(video, size):\n",
    "    video_list = tf.unstack(video, axis=1)\n",
    "    for i in range(len(video_list)):\n",
    "        video_list[i] = tf.image.resize(\n",
    "            video_list[i],\n",
    "            size\n",
    "        )\n",
    "    video = tf.stack(video_list, axis=1)\n",
    "    return video\n",
    "\n",
    "\n",
    "class TrainingPreprocessing(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        crop_size=(CROP_SIZE, CROP_SIZE),\n",
    "        image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.crop_size = crop_size\n",
    "        self.image_size = image_size\n",
    "        super(TrainingPreprocessing, self).__init__()\n",
    "\n",
    "    def call(self, data):\n",
    "        video = data\n",
    "        video = random_crop(video, self.crop_size)\n",
    "        video = resize(video, self.image_size)\n",
    "        sample = tf.random.uniform(shape=[], minval=0, maxval=1, dtype=tf.float32)\n",
    "        option = tf.less(sample, 0.5)\n",
    "        video= tf.cond(\n",
    "            option,\n",
    "            lambda: left_right_flip(video),\n",
    "            lambda: video\n",
    "        )\n",
    "        video = tf.cast(video, tf.float32) * (1 / 255.)\n",
    "        return video\n",
    "\n",
    "\n",
    "class TestingPreprocessing(tf.keras.layers.Layer):\n",
    "    def __init__(self, size=(IMAGE_SIZE, IMAGE_SIZE), **kwargs):\n",
    "        self.size = size\n",
    "        super(TestingPreprocessing, self).__init__()\n",
    "\n",
    "    def call(self, data):\n",
    "        video = data\n",
    "        video = resize(video, self.size)\n",
    "        video = tf.cast(video, tf.float32) * (1 / 255.)\n",
    "        return video\n",
    "\n",
    "\n",
    "def get_train_augmentation_model(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    crop_size=(CROP_SIZE, CROP_SIZE),\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "):\n",
    "    inputs = keras.Input(\n",
    "        shape=input_shape,\n",
    "        name=\"Original Video\"\n",
    "    )\n",
    "    aug = TrainingPreprocessing(\n",
    "        crop_size=crop_size,\n",
    "        image_size=image_size\n",
    "    )(inputs)\n",
    "    return keras.Model(inputs=[inputs], outputs=[aug], name=\"train_data_augmentation\")\n",
    "\n",
    "\n",
    "def get_test_augmentation_model(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "):\n",
    "    inputs = keras.Input(\n",
    "        shape=input_shape,\n",
    "        name=\"Original Video\"\n",
    "    )\n",
    "    aug = TestingPreprocessing(\n",
    "        image_size=image_size\n",
    "    )(inputs)\n",
    "    return keras.Model(inputs=[inputs], outputs=[aug], name=\"test_data_augmentation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ef2621-b803-4091-ba90-e40f930f195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size=PATCH_SIZE, time_len=TIME_LEN, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.time_len = time_len\n",
    "        self.resize = layers.Reshape((-1, patch_size * patch_size * 3))\n",
    "\n",
    "    def call(self, data):\n",
    "        # video, label = data\n",
    "        video = data\n",
    "        # Create patches from the input images\n",
    "        video_list = tf.unstack(video, axis=1)\n",
    "        for i in range(len(video_list)):\n",
    "            patches = tf.image.extract_patches(\n",
    "                images=video_list[i],\n",
    "                sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "                strides=[1, self.patch_size, self.patch_size, 1],\n",
    "                rates=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "            )\n",
    "\n",
    "            # Reshape the patches to (batch, num_patches, patch_area) and return it.\n",
    "            video_list[i] = self.resize(patches)\n",
    "        video = tf.stack(video_list, axis=1)\n",
    "        # return video, label\n",
    "        return video\n",
    "\n",
    "    def show_patched_image(self, video, patches):\n",
    "        # This is a utility function which accepts a batch of images and its\n",
    "        # corresponding patches and help visualize one image and its patches\n",
    "        # side by side.\n",
    "        idx = np.random.choice(patches.shape[0])\n",
    "        n = int(np.sqrt(patches.shape[-2]))\n",
    "        print(f\"Index selected: {idx}.\")\n",
    "\n",
    "        fig = plt.figure()\n",
    "        gs = gridspec.GridSpec(n, 2 * n, figure=fig, hspace=0.08, wspace=0.1)\n",
    "        ax = fig.add_subplot(gs[:n, n: 2*n])\n",
    "        big_im = ax.imshow(video[idx, 0, ...])\n",
    "        ax.set_axis_off()\n",
    "\n",
    "        grid = list(product(range(n), range(n)))\n",
    "        patch_list = []\n",
    "        for i in range(patches.shape[-2]):\n",
    "            ax = fig.add_subplot(gs[grid[i][0], grid[i][1]])\n",
    "            patch_img = tf.reshape(\n",
    "                patches[idx, 0, i, :],\n",
    "                (self.patch_size, self.patch_size, 3)\n",
    "            )\n",
    "            im = ax.imshow(patch_img)\n",
    "            patch_list.append(im)\n",
    "            ax.set_axis_off()\n",
    "        plt.close()\n",
    "        def init():\n",
    "            big_im.set_data(video[idx,0,...])\n",
    "            for i, im in enumerate(patch_list):\n",
    "                patch_img = tf.reshape(\n",
    "                    patches[idx, 0, i, :],\n",
    "                    (self.patch_size, self.patch_size, 3)\n",
    "                )\n",
    "                im.set_data(patch_img)\n",
    "        def animate(j):\n",
    "            big_im.set_data(video[idx,j,...])\n",
    "            for i, im in enumerate(patch_list):\n",
    "                patch_img = tf.reshape(\n",
    "                    patches[idx, j, i, :],\n",
    "                    (self.patch_size, self.patch_size, 3)\n",
    "                )\n",
    "                im.set_data(patch_img)\n",
    "        anim = animation.FuncAnimation(\n",
    "            fig,\n",
    "            animate,\n",
    "            init_func=init,\n",
    "            frames=video.shape[1],\n",
    "            interval=50\n",
    "        )\n",
    "        return anim, idx\n",
    "\n",
    "    # taken from https://stackoverflow.com/a/58082878/10319735\n",
    "    def reconstruct_from_patch(self, patch):\n",
    "        # This utility function takes patches from a *single* image and\n",
    "        # reconstructs it back into the image. This is useful for the train\n",
    "        # monitor callback.\n",
    "        num_patches = patch.shape[-2]\n",
    "        n = int(np.sqrt(num_patches))\n",
    "        patch = tf.reshape(patch, (self.time_len, num_patches, self.patch_size, self.patch_size, 3))\n",
    "        video = []\n",
    "        for i in range(self.time_len):\n",
    "            rows = tf.split(patch[i], n, axis=0)\n",
    "            rows = [tf.concat(tf.unstack(x), axis=1) for x in rows]\n",
    "            reconstructed = tf.concat(rows, axis=0)\n",
    "            video.append(reconstructed)\n",
    "        return tf.stack(video, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae03f899-a853-4ca6-aa06-902fa1f7b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        time_len=TIME_LEN,\n",
    "        projection_dim=ENC_PROJECTION_DIM,\n",
    "        mask_proportion=MASK_PROPORTION,\n",
    "        downstream=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.time_len = time_len\n",
    "        self.projection_dim = projection_dim\n",
    "        self.mask_proportion = mask_proportion\n",
    "        self.downstream = downstream\n",
    "\n",
    "        # This is a trainable mask token initialized randomly from a normal\n",
    "        # distribution.\n",
    "        self.mask_token = tf.Variable(\n",
    "            tf.random.normal([self.time_len, 1, patch_size * patch_size * 3]), trainable=True\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        (_, self.num_frames, self.num_patches, self.patch_area) = input_shape\n",
    "\n",
    "        # Create the projection layer for the patches.\n",
    "        self.projection = layers.GRU(units=self.projection_dim)\n",
    "        # Create the positional embedding layer.\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=self.num_patches, output_dim=self.projection_dim\n",
    "        )\n",
    "\n",
    "        # Number of patches that will be masked.\n",
    "        self.num_mask = int(self.mask_proportion * self.num_patches)\n",
    "\n",
    "    def call(self, patches):\n",
    "        # patches: (B, T, N, ps*ps)\n",
    "        # Get the positional embeddings.\n",
    "        batch_size = tf.shape(patches)[0]\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        pos_embeddings = self.position_embedding(positions[tf.newaxis, ...])\n",
    "        pos_embeddings = tf.tile(\n",
    "            pos_embeddings, [batch_size, 1, 1]\n",
    "        )  # (B, num_patches, projection_dim)\n",
    "\n",
    "        # Embed the patches. (GRU)\n",
    "        projection = tf.unstack(patches, axis=-2)\n",
    "        for i in range(len(projection)):\n",
    "            projection[i] = self.projection(projection[i])\n",
    "        # (B, num_patches, projection_dim)\n",
    "        projection = tf.stack(projection, axis=1)\n",
    "\n",
    "        patch_embeddings = (\n",
    "            projection + pos_embeddings\n",
    "        )  # (B, num_patches, projection_dim)\n",
    "\n",
    "        if self.downstream:\n",
    "            return patch_embeddings\n",
    "        else:\n",
    "            mask_indices, unmask_indices = self.get_random_indices(batch_size)\n",
    "            # The encoder input is the unmasked patch embeddings. Here we gather\n",
    "            # all the patches that should be unmasked.\n",
    "            unmasked_embeddings = tf.gather(\n",
    "                patch_embeddings, unmask_indices, axis=1, batch_dims=1\n",
    "            )  # (B, unmask_numbers, projection_dim)\n",
    "\n",
    "            # Get the unmasked and masked position embeddings. We will need them\n",
    "            # for the decoder.\n",
    "            unmasked_positions = tf.gather(\n",
    "                pos_embeddings, unmask_indices, axis=1, batch_dims=1\n",
    "            )  # (B, unmask_numbers, projection_dim)\n",
    "            masked_positions = tf.gather(\n",
    "                pos_embeddings, mask_indices, axis=1, batch_dims=1\n",
    "            )  # (B, mask_numbers, projection_dim)\n",
    "\n",
    "            # Repeat the mask token number of mask times.\n",
    "            # Mask tokens replace the masks of the image.\n",
    "            # mask_tokens: (T, ps*ps)\n",
    "            mask_tokens = tf.repeat(self.mask_token, repeats=self.num_mask, axis=1)\n",
    "            # mask_tokens = (mask_numbers, projection_dim) \n",
    "            mask_tokens = tf.repeat(\n",
    "                mask_tokens[tf.newaxis, ...], repeats=batch_size, axis=0\n",
    "            )\n",
    "            # Embed the tokens (GRU)\n",
    "            mask_tokens = tf.unstack(mask_tokens, axis=-2)\n",
    "            for i in range(len(mask_tokens)):\n",
    "                mask_tokens[i] = self.projection(mask_tokens[i])\n",
    "            # (B, num_patches, projection_dim)\n",
    "            mask_tokens = tf.stack(mask_tokens, axis=1)\n",
    "            # Get the masked embeddings for the tokens.\n",
    "            masked_embeddings = mask_tokens + masked_positions\n",
    "            return (\n",
    "                unmasked_embeddings,  # Input to the encoder.\n",
    "                masked_embeddings,  # First part of input to the decoder.\n",
    "                unmasked_positions,  # Added to the encoder outputs.\n",
    "                mask_indices,  # The indices that were masked.\n",
    "                unmask_indices,  # The indices that were unmaksed.\n",
    "            )\n",
    "\n",
    "    def get_random_indices(self, batch_size):\n",
    "        # Create random indices from a uniform distribution and then split\n",
    "        # it into mask and unmask indices.\n",
    "        rand_indices = tf.argsort(\n",
    "            tf.random.uniform(shape=(batch_size, self.num_patches)), axis=-1\n",
    "        )\n",
    "        mask_indices = rand_indices[:, : self.num_mask]\n",
    "        unmask_indices = rand_indices[:, self.num_mask :]\n",
    "        return mask_indices, unmask_indices\n",
    "\n",
    "    def generate_masked_image(self, patches, unmask_indices):\n",
    "        # Choose a random patch and it corresponding unmask index.\n",
    "        idx = np.random.choice(patches.shape[0])\n",
    "        patch = patches[idx]\n",
    "        unmask_index = unmask_indices[idx]\n",
    "\n",
    "        # Build a numpy array of same shape as patch.\n",
    "        new_patch = np.zeros_like(patch)\n",
    "\n",
    "        # Iterate of the new_patch and plug the unmasked patches.\n",
    "        for i in range(unmask_index.shape[0]):\n",
    "            new_patch[:, unmask_index[i], ...] = patch[:, unmask_index[i], ...]\n",
    "        return new_patch, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36410e77-f40c-4d2f-b364-e46dd3cec525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, dropout_rate, hidden_units):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_encoder(\n",
    "    num_heads=ENC_NUM_HEADS,\n",
    "    num_layers=ENC_LAYERS,\n",
    "    projection_dim=ENC_PROJECTION_DIM,\n",
    "    transformer_units=ENC_TRANSFORMER_UNITS,\n",
    "    epsilon=LAYER_NORM_EPS,\n",
    "):\n",
    "    inputs = layers.Input(\n",
    "        (None, projection_dim)\n",
    "    )\n",
    "    x = inputs\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=epsilon)(x)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, x])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=epsilon)(x2)\n",
    "\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add()([x3, x2])\n",
    "\n",
    "    outputs = layers.LayerNormalization(epsilon=epsilon)(x)\n",
    "    return keras.Model(inputs, outputs, name=\"mae_encoder\")\n",
    "\n",
    "\n",
    "def create_decoder(\n",
    "    num_layers=DEC_LAYERS,\n",
    "    num_heads=DEC_NUM_HEADS,\n",
    "    num_patches=NUM_PATCHES,\n",
    "    enc_projection_dim=ENC_PROJECTION_DIM,\n",
    "    dec_projection_dim=DEC_PROJECTION_DIM,\n",
    "    epsilon=LAYER_NORM_EPS,\n",
    "    transformer_units=DEC_TRANSFORMER_UNITS,\n",
    "    image_size=IMAGE_SIZE\n",
    "):\n",
    "    inputs = layers.Input(\n",
    "        (num_patches, enc_projection_dim)\n",
    "    )\n",
    "    x = layers.Dense(dec_projection_dim)(inputs)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=epsilon)(x)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=dec_projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, x])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=epsilon)(x2)\n",
    "\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add()([x3, x2])\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=epsilon)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    pre_final = layers.Dense(units=image_size * image_size * 3, activation='sigmoid')(x) # tanh sigmoid\n",
    "    outputs = layers.Reshape((image_size, image_size, 3))(pre_final)\n",
    "\n",
    "    return keras.Model(inputs, outputs, name=\"mae_decoder\")\n",
    "\n",
    "\n",
    "class MaskedAutoencoder(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape=INPUT_SHAPE,\n",
    "        output_shape=OUTPUT_SHAPE,\n",
    "        crop_size=(CROP_SIZE, CROP_SIZE),\n",
    "        image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        patch_size=PATCH_SIZE,\n",
    "        num_patches=NUM_PATCHES,\n",
    "        time_len=TIME_LEN,\n",
    "        mask_proportion=MASK_PROPORTION,\n",
    "        enc_projection_dim=ENC_PROJECTION_DIM,\n",
    "        enc_transformer_units=ENC_TRANSFORMER_UNITS,\n",
    "        num_enc_heads=ENC_NUM_HEADS,\n",
    "        num_enc_layers=ENC_LAYERS,\n",
    "        num_dec_layers=DEC_LAYERS,\n",
    "        num_dec_heads=DEC_NUM_HEADS,\n",
    "        dec_projection_dim=DEC_PROJECTION_DIM,\n",
    "        dec_transformer_units=DEC_TRANSFORMER_UNITS,\n",
    "        epsilon=LAYER_NORM_EPS,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_augmentation_model = get_train_augmentation_model(\n",
    "            input_shape=input_shape,\n",
    "            crop_size=crop_size,\n",
    "            image_size=image_size,\n",
    "        )\n",
    "        self.test_augmentation_model = get_test_augmentation_model(\n",
    "            input_shape=input_shape,\n",
    "            image_size=image_size,\n",
    "        )\n",
    "        self.patch_layer = Patches(\n",
    "            patch_size=patch_size,\n",
    "            time_len=time_len,\n",
    "        )\n",
    "        self.patch_encoder = PatchEncoder(\n",
    "            patch_size=patch_size,\n",
    "            time_len=time_len,\n",
    "            projection_dim=enc_projection_dim,\n",
    "            mask_proportion=mask_proportion,\n",
    "            downstream=False\n",
    "        )\n",
    "        self.encoder = create_encoder(\n",
    "            num_heads=num_enc_heads,\n",
    "            num_layers=num_enc_layers,\n",
    "            projection_dim=enc_projection_dim,\n",
    "            transformer_units=enc_transformer_units,\n",
    "            epsilon=epsilon,\n",
    "        )\n",
    "        self.decoder = create_decoder(\n",
    "            num_layers=num_dec_layers,\n",
    "            num_heads=num_dec_heads,\n",
    "            num_patches=num_patches,\n",
    "            enc_projection_dim=enc_projection_dim,\n",
    "            dec_projection_dim=dec_projection_dim,\n",
    "            epsilon=epsilon,\n",
    "            transformer_units=dec_transformer_units,\n",
    "            image_size=image_size[0],\n",
    "        )\n",
    "        self.resize = layers.Reshape((-1, patch_size * patch_size * 1))\n",
    "        self.mse_loss = tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\")\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # Encode the patches.\n",
    "        (\n",
    "            unmasked_embeddings,\n",
    "            masked_embeddings,\n",
    "            unmasked_positions,\n",
    "            mask_indices,\n",
    "            unmask_indices,\n",
    "        ) = self.patch_encoder(inputs)\n",
    "\n",
    "        # Pass the unmaksed patche to the encoder.\n",
    "        encoder_outputs = self.encoder(unmasked_embeddings)\n",
    "\n",
    "        # Create the decoder inputs.\n",
    "        encoder_outputs = encoder_outputs + unmasked_positions\n",
    "        decoder_inputs = tf.concat([encoder_outputs, masked_embeddings], axis=1)\n",
    "\n",
    "        # Decode the inputs.\n",
    "        decoder_outputs = self.decoder(decoder_inputs)\n",
    "        return decoder_outputs\n",
    "\n",
    "    def train_step(self, data):\n",
    "        videos, next_frames = data\n",
    "        aug_videos, next_frame = self.train_augmentation_model([videos, next_frames])\n",
    "        # Patch the augmented images.\n",
    "        vid_patches, frame_patches = self.patch_layer([aug_videos, next_frame])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            decoder_outputs = self(vid_patches)\n",
    "            decoder_patches = tf.image.extract_patches(\n",
    "                images=decoder_outputs,\n",
    "                sizes=[1, self.patch_layer.patch_size, self.patch_layer.patch_size, 1],\n",
    "                strides=[1, self.patch_layer.patch_size, self.patch_layer.patch_size, 1],\n",
    "                rates=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "            )\n",
    "            # Calculate loss on all patches.\n",
    "            loss_output = self.resize(decoder_patches)\n",
    "            loss_patch = self.resize(frame_patches)\n",
    "            # Calculate loss on masked patches.\n",
    "            # loss_patch = tf.gather(\n",
    "            #     loss_patch,\n",
    "            #     mask_indices,\n",
    "            #     axis=1,\n",
    "            #     batch_dims=1\n",
    "            # )\n",
    "            # loss_output = tf.gather(\n",
    "            #     loss_output,\n",
    "            #     mask_indices,\n",
    "            #     axis=1,\n",
    "            #     batch_dims=1\n",
    "            # )\n",
    "            # Compute the total loss.\n",
    "            # Calculate loss on masked patches\n",
    "            # total_loss = self.compiled_loss(loss_patch, loss_output)\n",
    "            # # Calculate loss on all outputs\n",
    "            total_loss = self.mse_loss(frame_patches, decoder_patches)\n",
    "        # Apply gradients.\n",
    "        train_vars = [\n",
    "            self.train_augmentation_model.trainable_variables,\n",
    "            self.patch_layer.trainable_variables,\n",
    "            self.patch_encoder.trainable_variables,\n",
    "            self.encoder.trainable_variables,\n",
    "            self.decoder.trainable_variables,\n",
    "        ]\n",
    "        grads = tape.gradient(total_loss, train_vars)\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        tv_list = []\n",
    "        for (grad, var) in zip(grads, train_vars):\n",
    "            for g, v in zip(grad, var):\n",
    "                tv_list.append((g, v))\n",
    "        self.optimizer.apply_gradients(tv_list)\n",
    "\n",
    "        # Report progress.\n",
    "\n",
    "        self.loss_tracker.update_state(total_loss)\n",
    "        self.mae_metric.update_state(loss_patch, loss_output)\n",
    "        return {\"loss\": self.loss_tracker.result(), \"mae\": self.mae_metric.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        videos, next_frames = data\n",
    "        aug_videos, next_frame = self.test_augmentation_model([videos, next_frames])\n",
    "        vid_patches, frame_patches = self.patch_layer([aug_videos, next_frame])\n",
    "\n",
    "        decoder_outputs = self(vid_patches)\n",
    "        decoder_patches = tf.image.extract_patches(\n",
    "            images=decoder_outputs,\n",
    "            sizes=[1, self.patch_layer.patch_size, self.patch_layer.patch_size, 1],\n",
    "            strides=[1, self.patch_layer.patch_size, self.patch_layer.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        # Calculate loss on all patches.\n",
    "        loss_output = self.resize(decoder_patches)\n",
    "        loss_patch = self.resize(frame_patches)\n",
    "        # Calculate loss on masked patches.\n",
    "        # loss_patch = tf.gather(\n",
    "        #     loss_patch,\n",
    "        #     mask_indices,\n",
    "        #     axis=1,\n",
    "        #     batch_dims=1\n",
    "        # )\n",
    "        # loss_output = tf.gather(\n",
    "        #     loss_output,\n",
    "        #     mask_indices,\n",
    "        #     axis=1,\n",
    "        #     batch_dims=1\n",
    "        # )\n",
    "        # Compute the total loss.\n",
    "        # Calculate loss on masked patches\n",
    "        # total_loss = self.compiled_loss(loss_patch, loss_output)\n",
    "        # # Calculate loss on all outputs\n",
    "        total_loss = self.mse_loss(frame_patches, decoder_patches)\n",
    "        self.loss_tracker.update_state(total_loss)\n",
    "        self.mae_metric.update_state(loss_patch, loss_output)\n",
    "        return {\"loss\": self.loss_tracker.result(), \"mae\": self.mae_metric.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [self.loss_tracker, self.mae_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1145826-f55a-42e0-98b6-31f3e3cf77cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 06:31:10.965196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-17 06:31:10.966015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-17 06:31:10.976207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-17 06:31:10.977032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-17 06:31:10.978584: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-17 06:31:11.185658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-17 06:31:11.186383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-17 06:31:11.187061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-17 06:31:11.187725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-17 06:31:12.061917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-17 06:31:12.062894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-17 06:31:12.063610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-17 06:31:12.064239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-17 06:31:12.064927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-17 06:31:12.065564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13823 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2022-05-17 06:31:12.066242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-17 06:31:12.066917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13823 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n",
    "    ):\n",
    "        super(WarmUpCosine, self).__init__()\n",
    "\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.pi = tf.constant(np.pi)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        if self.total_steps < self.warmup_steps:\n",
    "            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n",
    "\n",
    "        cos_annealed_lr = tf.cos(\n",
    "            self.pi\n",
    "            * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
    "            / float(self.total_steps - self.warmup_steps)\n",
    "        )\n",
    "        learning_rate = 0.5 * self.learning_rate_base * (1 + cos_annealed_lr)\n",
    "\n",
    "        if self.warmup_steps > 0:\n",
    "            if self.learning_rate_base < self.warmup_learning_rate:\n",
    "                raise ValueError(\n",
    "                    \"Learning_rate_base must be larger or equal to \"\n",
    "                    \"warmup_learning_rate.\"\n",
    "                )\n",
    "            slope = (\n",
    "                self.learning_rate_base - self.warmup_learning_rate\n",
    "            ) / self.warmup_steps\n",
    "            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps, warmup_rate, learning_rate\n",
    "            )\n",
    "        return tf.where(\n",
    "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
    "        )\n",
    "\n",
    "class CollectGarbage(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "total_steps = int((3630 / BATCH_SIZE) * EPOCHS)\n",
    "warmup_epoch_percentage = 0.1\n",
    "warmup_steps = int(total_steps * warmup_epoch_percentage)\n",
    "scheduled_lrs = WarmUpCosine(\n",
    "    learning_rate_base=0.1,\n",
    "    total_steps=total_steps,\n",
    "    warmup_learning_rate=0.0,\n",
    "    warmup_steps=warmup_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0bcb73a-0733-40ff-b60e-b179bae78bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 06:32:49.298212: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 132s 816ms/step - loss: 1.7835 - accuracy: 0.2112 - val_loss: 4.1798 - val_accuracy: 0.1968\n",
      "Epoch 2/50\n",
      "56/56 [==============================] - 58s 633ms/step - loss: 1.7203 - accuracy: 0.2567 - val_loss: 4.3691 - val_accuracy: 0.1980\n",
      "Epoch 3/50\n",
      "56/56 [==============================] - 59s 643ms/step - loss: 1.6720 - accuracy: 0.2810 - val_loss: 2.3625 - val_accuracy: 0.2209\n",
      "Epoch 4/50\n",
      "56/56 [==============================] - 57s 621ms/step - loss: 1.5413 - accuracy: 0.3092 - val_loss: 3.9361 - val_accuracy: 0.1883\n",
      "Epoch 5/50\n",
      "56/56 [==============================] - 57s 619ms/step - loss: 1.4505 - accuracy: 0.3259 - val_loss: 5.0787 - val_accuracy: 0.1245\n",
      "Epoch 6/50\n",
      "56/56 [==============================] - 58s 633ms/step - loss: 1.4078 - accuracy: 0.3465 - val_loss: 1.7351 - val_accuracy: 0.2566\n",
      "Epoch 7/50\n",
      "56/56 [==============================] - 57s 619ms/step - loss: 1.3315 - accuracy: 0.3594 - val_loss: 4.2651 - val_accuracy: 0.2193\n",
      "Epoch 8/50\n",
      "56/56 [==============================] - 58s 626ms/step - loss: 1.3032 - accuracy: 0.3605 - val_loss: 1.6537 - val_accuracy: 0.3342\n",
      "Epoch 9/50\n",
      "56/56 [==============================] - 57s 622ms/step - loss: 1.2699 - accuracy: 0.3728 - val_loss: 5.4505 - val_accuracy: 0.1863\n",
      "Epoch 10/50\n",
      "56/56 [==============================] - 57s 626ms/step - loss: 1.2428 - accuracy: 0.3753 - val_loss: 1.8128 - val_accuracy: 0.3942\n",
      "Epoch 11/50\n",
      "56/56 [==============================] - 58s 626ms/step - loss: 1.1438 - accuracy: 0.4127 - val_loss: 1.3509 - val_accuracy: 0.3984\n",
      "Epoch 12/50\n",
      "56/56 [==============================] - 57s 624ms/step - loss: 1.1264 - accuracy: 0.4155 - val_loss: 1.4252 - val_accuracy: 0.3889\n",
      "Epoch 13/50\n",
      "56/56 [==============================] - 57s 626ms/step - loss: 1.1201 - accuracy: 0.4180 - val_loss: 3.1792 - val_accuracy: 0.2247\n",
      "Epoch 14/50\n",
      "56/56 [==============================] - 58s 626ms/step - loss: 1.1113 - accuracy: 0.4199 - val_loss: 1.3235 - val_accuracy: 0.4245\n",
      "Epoch 15/50\n",
      "56/56 [==============================] - 58s 625ms/step - loss: 1.0983 - accuracy: 0.4219 - val_loss: 1.2771 - val_accuracy: 0.3912\n",
      "Epoch 16/50\n",
      "56/56 [==============================] - 58s 629ms/step - loss: 1.1195 - accuracy: 0.4272 - val_loss: 1.2603 - val_accuracy: 0.4252\n",
      "Epoch 17/50\n",
      "56/56 [==============================] - 57s 620ms/step - loss: 1.0839 - accuracy: 0.4428 - val_loss: 1.8982 - val_accuracy: 0.2426\n",
      "Epoch 18/50\n",
      "56/56 [==============================] - 59s 648ms/step - loss: 1.0794 - accuracy: 0.4408 - val_loss: 1.3352 - val_accuracy: 0.4210\n",
      "Epoch 19/50\n",
      "56/56 [==============================] - 58s 625ms/step - loss: 1.0489 - accuracy: 0.4824 - val_loss: 1.2645 - val_accuracy: 0.4313\n",
      "Epoch 20/50\n",
      "56/56 [==============================] - 58s 624ms/step - loss: 1.0534 - accuracy: 0.4791 - val_loss: 1.5196 - val_accuracy: 0.4031\n",
      "Epoch 21/50\n",
      "56/56 [==============================] - 59s 635ms/step - loss: 1.0203 - accuracy: 0.5075 - val_loss: 3.2209 - val_accuracy: 0.2640\n",
      "Epoch 22/50\n",
      "56/56 [==============================] - 59s 628ms/step - loss: 1.0745 - accuracy: 0.4919 - val_loss: 2.2293 - val_accuracy: 0.4219\n",
      "Epoch 23/50\n",
      "56/56 [==============================] - 61s 643ms/step - loss: 0.9560 - accuracy: 0.5558 - val_loss: 1.4601 - val_accuracy: 0.4435\n",
      "Epoch 24/50\n",
      "56/56 [==============================] - 59s 628ms/step - loss: 0.9140 - accuracy: 0.5706 - val_loss: 1.1575 - val_accuracy: 0.5130\n",
      "Epoch 25/50\n",
      "56/56 [==============================] - 58s 627ms/step - loss: 0.8966 - accuracy: 0.5890 - val_loss: 1.3020 - val_accuracy: 0.4548\n",
      "Epoch 26/50\n",
      "56/56 [==============================] - 58s 625ms/step - loss: 0.8910 - accuracy: 0.5904 - val_loss: 1.9599 - val_accuracy: 0.3901\n",
      "Epoch 27/50\n",
      "56/56 [==============================] - 60s 653ms/step - loss: 0.8418 - accuracy: 0.6236 - val_loss: 1.4212 - val_accuracy: 0.4698\n",
      "Epoch 28/50\n",
      "56/56 [==============================] - 59s 621ms/step - loss: 0.8472 - accuracy: 0.6133 - val_loss: 1.5859 - val_accuracy: 0.4791\n",
      "Epoch 29/50\n",
      "56/56 [==============================] - 58s 625ms/step - loss: 0.8138 - accuracy: 0.6381 - val_loss: 2.3059 - val_accuracy: 0.3876\n",
      "Epoch 30/50\n",
      "56/56 [==============================] - 59s 628ms/step - loss: 0.8190 - accuracy: 0.6334 - val_loss: 1.7863 - val_accuracy: 0.4309\n",
      "Epoch 31/50\n",
      "56/56 [==============================] - 58s 616ms/step - loss: 0.8413 - accuracy: 0.6214 - val_loss: 1.3051 - val_accuracy: 0.5165\n",
      "Epoch 32/50\n",
      "56/56 [==============================] - 58s 626ms/step - loss: 0.8312 - accuracy: 0.6367 - val_loss: 1.4096 - val_accuracy: 0.4934\n",
      "Epoch 33/50\n",
      "56/56 [==============================] - 58s 621ms/step - loss: 0.8000 - accuracy: 0.6529 - val_loss: 1.2692 - val_accuracy: 0.5056\n",
      "Epoch 34/50\n",
      "56/56 [==============================] - 58s 622ms/step - loss: 0.8074 - accuracy: 0.6364 - val_loss: 2.6126 - val_accuracy: 0.3503\n",
      "Epoch 35/50\n",
      "56/56 [==============================] - 58s 625ms/step - loss: 0.7851 - accuracy: 0.6621 - val_loss: 1.6269 - val_accuracy: 0.5248\n",
      "Epoch 36/50\n",
      "56/56 [==============================] - 58s 623ms/step - loss: 0.7718 - accuracy: 0.6635 - val_loss: 1.4624 - val_accuracy: 0.5047\n",
      "Epoch 37/50\n",
      "56/56 [==============================] - 58s 628ms/step - loss: 0.7340 - accuracy: 0.6755 - val_loss: 1.4242 - val_accuracy: 0.5074\n",
      "Epoch 38/50\n",
      "56/56 [==============================] - 58s 632ms/step - loss: 0.7675 - accuracy: 0.6596 - val_loss: 0.9736 - val_accuracy: 0.5890\n",
      "Epoch 39/50\n",
      "56/56 [==============================] - 58s 620ms/step - loss: 0.7713 - accuracy: 0.6702 - val_loss: 1.4403 - val_accuracy: 0.5406\n",
      "Epoch 40/50\n",
      "56/56 [==============================] - 58s 624ms/step - loss: 0.7524 - accuracy: 0.6738 - val_loss: 1.4830 - val_accuracy: 0.4904\n",
      "Epoch 41/50\n",
      "56/56 [==============================] - 58s 628ms/step - loss: 0.7271 - accuracy: 0.6744 - val_loss: 1.0848 - val_accuracy: 0.6029\n",
      "Epoch 42/50\n",
      "56/56 [==============================] - 58s 625ms/step - loss: 0.7448 - accuracy: 0.6744 - val_loss: 1.4704 - val_accuracy: 0.5219\n",
      "Epoch 43/50\n",
      "56/56 [==============================] - 58s 626ms/step - loss: 0.7725 - accuracy: 0.6696 - val_loss: 1.2556 - val_accuracy: 0.5215\n",
      "Epoch 44/50\n",
      "56/56 [==============================] - 58s 619ms/step - loss: 0.7554 - accuracy: 0.6752 - val_loss: 1.2810 - val_accuracy: 0.5624\n",
      "Epoch 45/50\n",
      "56/56 [==============================] - 58s 625ms/step - loss: 0.7242 - accuracy: 0.6925 - val_loss: 1.9017 - val_accuracy: 0.4920\n",
      "Epoch 46/50\n",
      "56/56 [==============================] - 58s 623ms/step - loss: 0.8341 - accuracy: 0.6214 - val_loss: 1.1641 - val_accuracy: 0.5449\n",
      "Epoch 47/50\n",
      "56/56 [==============================] - 58s 625ms/step - loss: 0.7912 - accuracy: 0.6568 - val_loss: 1.8841 - val_accuracy: 0.4366\n",
      "Epoch 48/50\n",
      "56/56 [==============================] - 58s 621ms/step - loss: 0.8027 - accuracy: 0.6454 - val_loss: 1.1541 - val_accuracy: 0.5532\n",
      "Epoch 49/50\n",
      "56/56 [==============================] - 58s 626ms/step - loss: 0.8428 - accuracy: 0.6328 - val_loss: 1.2185 - val_accuracy: 0.4728\n",
      "Epoch 50/50\n",
      "56/56 [==============================] - 58s 624ms/step - loss: 0.7920 - accuracy: 0.6551 - val_loss: 1.0110 - val_accuracy: 0.5784\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "with strategy.scope():\n",
    "\n",
    "    mae_model = MaskedAutoencoder(\n",
    "        input_shape=INPUT_SHAPE,\n",
    "        output_shape=OUTPUT_SHAPE,\n",
    "        crop_size=(CROP_SIZE, CROP_SIZE),\n",
    "        image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        patch_size=PATCH_SIZE,\n",
    "        num_patches=NUM_PATCHES,\n",
    "        time_len=TIME_LEN,\n",
    "        mask_proportion=MASK_PROPORTION,\n",
    "        enc_projection_dim=ENC_PROJECTION_DIM,\n",
    "        enc_transformer_units=ENC_TRANSFORMER_UNITS,\n",
    "        num_enc_heads=ENC_NUM_HEADS,\n",
    "        num_enc_layers=ENC_LAYERS,\n",
    "        num_dec_layers=DEC_LAYERS,\n",
    "        num_dec_heads=DEC_NUM_HEADS,\n",
    "        dec_projection_dim=DEC_PROJECTION_DIM,\n",
    "        dec_transformer_units=DEC_TRANSFORMER_UNITS,\n",
    "        epsilon=LAYER_NORM_EPS,\n",
    "    )\n",
    "    mae_model.load_weights('models/KTH_500/')\n",
    "\n",
    "    # Extract the augmentation layers.\n",
    "    train_augmentation_model = mae_model.train_augmentation_model\n",
    "    test_augmentation_model = mae_model.test_augmentation_model\n",
    "\n",
    "    # Extract the patchers.\n",
    "    patch_layer = mae_model.patch_layer\n",
    "    patch_encoder = mae_model.patch_encoder\n",
    "    patch_encoder.downstream = True  # Swtich the downstream flag to True.\n",
    "\n",
    "    # Extract the encoder.\n",
    "    encoder = mae_model.encoder\n",
    "\n",
    "    # Pack as a model.\n",
    "    downstream_model = keras.Sequential(\n",
    "        [\n",
    "            layers.Input((15, IMAGE_SIZE, IMAGE_SIZE, 3)),\n",
    "            patch_layer,\n",
    "            patch_encoder,\n",
    "            encoder,\n",
    "            layers.BatchNormalization(),  # Refer to A.1 (Linear probing).\n",
    "            layers.GlobalAveragePooling1D(),\n",
    "            layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n",
    "        ],\n",
    "        name=\"linear_probe_model\",\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Only the final classification layer of the `downstream_model` should be trainable.\n",
    "    # for layer in downstream_model.layers[:-1]:\n",
    "    #     layer.trainable = False\n",
    "    \n",
    "    optimizer = keras.optimizers.SGD(learning_rate=scheduled_lrs, momentum=0.9)\n",
    "    \n",
    "    # Compile and pretrain the model.\n",
    "    downstream_model.compile(\n",
    "        optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "\n",
    "batch_size = BATCH_SIZE * strategy.num_replicas_in_sync    \n",
    "\n",
    "files = tf.data.Dataset.list_files(\"datasets/KTH_tfrecords/training/*.tfrecord\")\n",
    "train_ds = files.interleave(\n",
    "    lambda x: tf.data.TFRecordDataset(x).prefetch(100),\n",
    "    cycle_length=8\n",
    ")\n",
    "train_ds = train_ds.map(parse, num_parallel_calls=AUTO)\n",
    "train_ds = train_ds.repeat()\n",
    "train_ds = (\n",
    "    train_ds\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(batch_size)\n",
    "    .map(\n",
    "            lambda x, y: (train_augmentation_model(x), y), num_parallel_calls=AUTO\n",
    "        )\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "\n",
    "files = tf.data.Dataset.list_files(\"datasets/KTH_tfrecords/validation/*.tfrecord\")\n",
    "val_ds = files.interleave(\n",
    "    lambda x: tf.data.TFRecordDataset(x).prefetch(100),\n",
    "    cycle_length=8\n",
    ")\n",
    "val_ds = val_ds.map(parse, num_parallel_calls=AUTO)\n",
    "val_ds = val_ds.repeat()\n",
    "val_ds = (\n",
    "    val_ds\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(batch_size)\n",
    "    .map(\n",
    "            lambda x, y: (test_augmentation_model(x) ,y), num_parallel_calls=AUTO\n",
    "        )\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='models/KTH_class/',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    save_format='tf',\n",
    ")\n",
    "\n",
    "\n",
    "train_callbacks = [\n",
    "    model_checkpoint_callback,\n",
    "    CollectGarbage(),\n",
    "]\n",
    "\n",
    "history = downstream_model.fit(\n",
    "    train_ds,\n",
    "    epochs=50,\n",
    "    validation_data=val_ds,\n",
    "    steps_per_epoch=3630 // batch_size,\n",
    "    validation_steps=7656 // batch_size,\n",
    "    callbacks=train_callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fd9597-8513-40e3-bdd1-b9d9a541c7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
