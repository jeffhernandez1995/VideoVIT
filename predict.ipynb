{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89374299-b30f-4a1d-9b65-56fd5fad56bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"]=\"2\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import matplotlib.gridspec as gridspec\n",
    "from IPython.display import HTML\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc75a23-7760-437f-ac57-709b65246c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seeds for reproducibility.\n",
    "SEED = 42\n",
    "keras.utils.set_random_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# DATA\n",
    "BUFFER_SIZE = 300\n",
    "BATCH_SIZE = 64\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (15, 120, 160, 3)\n",
    "TIME_LEN = INPUT_SHAPE[0]\n",
    "OUTPUT_SHAPE = (120, 160, 3)\n",
    "NUM_CLASSES = 6\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 5e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# PRETRAINING\n",
    "EPOCHS = 500\n",
    "\n",
    "# AUGMENTATION\n",
    "IMAGE_SIZE = 48  # We will resize input images to this size.\n",
    "PATCH_SIZE = 6  # Size of the patches to be extracted from the input images.\n",
    "CROP_SIZE = 100\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "MASK_PROPORTION = 0.75  # We have found 75% masking to give us the best results.\n",
    "\n",
    "# ENCODER and DECODER\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "ENC_PROJECTION_DIM = 128\n",
    "DEC_PROJECTION_DIM = 64\n",
    "ENC_NUM_HEADS = 4\n",
    "ENC_LAYERS = 8\n",
    "DEC_NUM_HEADS = 4\n",
    "DEC_LAYERS = (\n",
    "    4  # The decoder is lightweight but should be reasonably deep for reconstruction.\n",
    ")\n",
    "ENC_TRANSFORMER_UNITS = [\n",
    "    ENC_PROJECTION_DIM * 2,\n",
    "    ENC_PROJECTION_DIM,\n",
    "]  # Size of the transformer layers.\n",
    "DEC_TRANSFORMER_UNITS = [\n",
    "    DEC_PROJECTION_DIM * 2,\n",
    "    DEC_PROJECTION_DIM,\n",
    "]\n",
    "\n",
    "\n",
    "MIXED_PRECISION = False\n",
    "XLA_ACCELERATE = False\n",
    "\n",
    "if MIXED_PRECISION:\n",
    "    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
    "    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "    mixed_precision.set_policy(policy)\n",
    "    print('Mixed precision enabled')\n",
    "\n",
    "if XLA_ACCELERATE:\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    print('Accelerated Linear Algebra enabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa177011-9315-454e-9061-849a4c66438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Returns a Dataset for reading from a SageMaker PipeMode channel.\"\"\"\n",
    "features = {\n",
    "    'video': tf.io.FixedLenFeature([], tf.string),\n",
    "    'frame': tf.io.FixedLenFeature([], tf.string),\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "def parse(record):\n",
    "\n",
    "    parsed = tf.io.parse_single_example(\n",
    "        serialized=record,\n",
    "        features=features\n",
    "    )\n",
    "    video_raw = parsed['video']\n",
    "    video_raw = tf.io.decode_raw(video_raw, tf.uint8)\n",
    "    video_raw = tf.cast(video_raw, tf.float32)\n",
    "\n",
    "    label = parsed['label']\n",
    "    label = tf.cast(label, tf.float32)\n",
    "\n",
    "    video_raw = tf.reshape(video_raw, INPUT_SHAPE[:3] + (1,))\n",
    "    video_raw = tf.concat([video_raw, video_raw, video_raw], axis=-1)\n",
    "\n",
    "    return video_raw, label\n",
    "\n",
    "\n",
    "def left_right_flip(video):\n",
    "    '''\n",
    "    Performs tf.image.flip_left_right on entire list of video frames.\n",
    "    Work around since the random selection must be consistent for entire video\n",
    "    :param video: Tensor constaining video frames (N,H,W,3)\n",
    "    :return: video: Tensor constaining video frames left-right flipped (N,H,W,3)\n",
    "    '''\n",
    "    video_list = tf.unstack(video, axis=1)\n",
    "    for i in range(len(video_list)):\n",
    "        video_list[i] = tf.image.flip_left_right(video_list[i])\n",
    "    video = tf.stack(video_list, axis=1)\n",
    "    return video\n",
    "\n",
    "\n",
    "def random_crop(video, size):\n",
    "    # (T, H, W, 3)\n",
    "    shape = tf.shape(video)\n",
    "    size = tf.convert_to_tensor(size, dtype=shape.dtype)\n",
    "    h_diff = shape[2] - size[1]\n",
    "    w_diff = shape[3] - size[0]\n",
    "\n",
    "    dtype = shape.dtype\n",
    "    rands = tf.random.uniform(shape=[2], minval=0, maxval=dtype.max, dtype=dtype)\n",
    "    h_start = tf.cast(rands[0] % (h_diff + 1), dtype)\n",
    "    w_start = tf.cast(rands[1] % (w_diff + 1), dtype)\n",
    "    size = tf.cast(size, tf.int32)\n",
    "    video_list = tf.unstack(video, axis=1)\n",
    "    for i in range(len(video_list)):\n",
    "        video_list[i] = tf.image.crop_to_bounding_box(\n",
    "            video_list[i],\n",
    "            h_start, w_start,\n",
    "            size[1], size[0]\n",
    "        )\n",
    "    video = tf.stack(video_list, axis=1)\n",
    "\n",
    "    return video\n",
    "\n",
    "\n",
    "def resize(video, size):\n",
    "    video_list = tf.unstack(video, axis=1)\n",
    "    for i in range(len(video_list)):\n",
    "        video_list[i] = tf.image.resize(\n",
    "            video_list[i],\n",
    "            size\n",
    "        )\n",
    "    video = tf.stack(video_list, axis=1)\n",
    "    return video\n",
    "\n",
    "\n",
    "class TrainingPreprocessing(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        crop_size=(CROP_SIZE, CROP_SIZE),\n",
    "        image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.crop_size = crop_size\n",
    "        self.image_size = image_size\n",
    "        super(TrainingPreprocessing, self).__init__()\n",
    "\n",
    "    def call(self, data):\n",
    "        video = data\n",
    "        video = random_crop(video, self.crop_size)\n",
    "        video = resize(video, self.image_size)\n",
    "        sample = tf.random.uniform(shape=[], minval=0, maxval=1, dtype=tf.float32)\n",
    "        option = tf.less(sample, 0.5)\n",
    "        video= tf.cond(\n",
    "            option,\n",
    "            lambda: left_right_flip(video),\n",
    "            lambda: video\n",
    "        )\n",
    "        video = tf.cast(video, tf.float32) * (1 / 255.)\n",
    "        return video\n",
    "\n",
    "\n",
    "class TestingPreprocessing(tf.keras.layers.Layer):\n",
    "    def __init__(self, size=(IMAGE_SIZE, IMAGE_SIZE), **kwargs):\n",
    "        self.size = size\n",
    "        super(TestingPreprocessing, self).__init__()\n",
    "\n",
    "    def call(self, data):\n",
    "        video = data\n",
    "        video = resize(video, self.size)\n",
    "        video = tf.cast(video, tf.float32) * (1 / 255.)\n",
    "        return video\n",
    "\n",
    "\n",
    "def get_train_augmentation_model(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    crop_size=(CROP_SIZE, CROP_SIZE),\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "):\n",
    "    inputs = keras.Input(\n",
    "        shape=input_shape,\n",
    "        name=\"Original Video\"\n",
    "    )\n",
    "    aug = TrainingPreprocessing(\n",
    "        crop_size=crop_size,\n",
    "        image_size=image_size\n",
    "    )(inputs)\n",
    "    return keras.Model(inputs=[inputs], outputs=[aug], name=\"train_data_augmentation\")\n",
    "\n",
    "\n",
    "def get_test_augmentation_model(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "):\n",
    "    inputs = keras.Input(\n",
    "        shape=input_shape,\n",
    "        name=\"Original Video\"\n",
    "    )\n",
    "    aug = TestingPreprocessing(\n",
    "        image_size=image_size\n",
    "    )(inputs)\n",
    "    return keras.Model(inputs=[inputs], outputs=[aug], name=\"test_data_augmentation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1256a1-c2dc-4437-a5fb-42c6b4ea5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size=PATCH_SIZE, time_len=TIME_LEN, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.time_len = time_len\n",
    "        self.resize = layers.Reshape((-1, patch_size * patch_size * 3))\n",
    "\n",
    "    def call(self, data):\n",
    "        # video, label = data\n",
    "        video = data\n",
    "        # Create patches from the input images\n",
    "        video_list = tf.unstack(video, axis=1)\n",
    "        for i in range(len(video_list)):\n",
    "            patches = tf.image.extract_patches(\n",
    "                images=video_list[i],\n",
    "                sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "                strides=[1, self.patch_size, self.patch_size, 1],\n",
    "                rates=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "            )\n",
    "\n",
    "            # Reshape the patches to (batch, num_patches, patch_area) and return it.\n",
    "            video_list[i] = self.resize(patches)\n",
    "        video = tf.stack(video_list, axis=1)\n",
    "        # return video, label\n",
    "        return video\n",
    "\n",
    "    def show_patched_image(self, video, patches):\n",
    "        # This is a utility function which accepts a batch of images and its\n",
    "        # corresponding patches and help visualize one image and its patches\n",
    "        # side by side.\n",
    "        idx = np.random.choice(patches.shape[0])\n",
    "        n = int(np.sqrt(patches.shape[-2]))\n",
    "        print(f\"Index selected: {idx}.\")\n",
    "\n",
    "        fig = plt.figure()\n",
    "        gs = gridspec.GridSpec(n, 2 * n, figure=fig, hspace=0.08, wspace=0.1)\n",
    "        ax = fig.add_subplot(gs[:n, n: 2*n])\n",
    "        big_im = ax.imshow(video[idx, 0, ...])\n",
    "        ax.set_axis_off()\n",
    "\n",
    "        grid = list(product(range(n), range(n)))\n",
    "        patch_list = []\n",
    "        for i in range(patches.shape[-2]):\n",
    "            ax = fig.add_subplot(gs[grid[i][0], grid[i][1]])\n",
    "            patch_img = tf.reshape(\n",
    "                patches[idx, 0, i, :],\n",
    "                (self.patch_size, self.patch_size, 3)\n",
    "            )\n",
    "            im = ax.imshow(patch_img)\n",
    "            patch_list.append(im)\n",
    "            ax.set_axis_off()\n",
    "        plt.close()\n",
    "        def init():\n",
    "            big_im.set_data(video[idx,0,...])\n",
    "            for i, im in enumerate(patch_list):\n",
    "                patch_img = tf.reshape(\n",
    "                    patches[idx, 0, i, :],\n",
    "                    (self.patch_size, self.patch_size, 3)\n",
    "                )\n",
    "                im.set_data(patch_img)\n",
    "        def animate(j):\n",
    "            big_im.set_data(video[idx,j,...])\n",
    "            for i, im in enumerate(patch_list):\n",
    "                patch_img = tf.reshape(\n",
    "                    patches[idx, j, i, :],\n",
    "                    (self.patch_size, self.patch_size, 3)\n",
    "                )\n",
    "                im.set_data(patch_img)\n",
    "        anim = animation.FuncAnimation(\n",
    "            fig,\n",
    "            animate,\n",
    "            init_func=init,\n",
    "            frames=video.shape[1],\n",
    "            interval=50\n",
    "        )\n",
    "        return anim, idx\n",
    "\n",
    "    # taken from https://stackoverflow.com/a/58082878/10319735\n",
    "    def reconstruct_from_patch(self, patch):\n",
    "        # This utility function takes patches from a *single* image and\n",
    "        # reconstructs it back into the image. This is useful for the train\n",
    "        # monitor callback.\n",
    "        num_patches = patch.shape[-2]\n",
    "        n = int(np.sqrt(num_patches))\n",
    "        patch = tf.reshape(patch, (self.time_len, num_patches, self.patch_size, self.patch_size, 3))\n",
    "        video = []\n",
    "        for i in range(self.time_len):\n",
    "            rows = tf.split(patch[i], n, axis=0)\n",
    "            rows = [tf.concat(tf.unstack(x), axis=1) for x in rows]\n",
    "            reconstructed = tf.concat(rows, axis=0)\n",
    "            video.append(reconstructed)\n",
    "        return tf.stack(video, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6af3e9d-1803-4203-be3f-0028409423ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        time_len=TIME_LEN,\n",
    "        projection_dim=ENC_PROJECTION_DIM,\n",
    "        mask_proportion=MASK_PROPORTION,\n",
    "        downstream=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.time_len = time_len\n",
    "        self.projection_dim = projection_dim\n",
    "        self.mask_proportion = mask_proportion\n",
    "        self.downstream = downstream\n",
    "\n",
    "        # This is a trainable mask token initialized randomly from a normal\n",
    "        # distribution.\n",
    "        self.mask_token = tf.Variable(\n",
    "            tf.random.normal([self.time_len, 1, patch_size * patch_size * 3]), trainable=True\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        (_, self.num_frames, self.num_patches, self.patch_area) = input_shape\n",
    "\n",
    "        # Create the projection layer for the patches.\n",
    "        self.projection = layers.GRU(units=self.projection_dim)\n",
    "        # Create the positional embedding layer.\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=self.num_patches, output_dim=self.projection_dim\n",
    "        )\n",
    "\n",
    "        # Number of patches that will be masked.\n",
    "        self.num_mask = int(self.mask_proportion * self.num_patches)\n",
    "\n",
    "    def call(self, patches):\n",
    "        # patches: (B, T, N, ps*ps)\n",
    "        # Get the positional embeddings.\n",
    "        batch_size = tf.shape(patches)[0]\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        pos_embeddings = self.position_embedding(positions[tf.newaxis, ...])\n",
    "        pos_embeddings = tf.tile(\n",
    "            pos_embeddings, [batch_size, 1, 1]\n",
    "        )  # (B, num_patches, projection_dim)\n",
    "\n",
    "        # Embed the patches. (GRU)\n",
    "        projection = tf.unstack(patches, axis=-2)\n",
    "        for i in range(len(projection)):\n",
    "            projection[i] = self.projection(projection[i])\n",
    "        # (B, num_patches, projection_dim)\n",
    "        projection = tf.stack(projection, axis=1)\n",
    "\n",
    "        patch_embeddings = (\n",
    "            projection + pos_embeddings\n",
    "        )  # (B, num_patches, projection_dim)\n",
    "\n",
    "        if self.downstream:\n",
    "            return patch_embeddings\n",
    "        else:\n",
    "            mask_indices, unmask_indices = self.get_random_indices(batch_size)\n",
    "            # The encoder input is the unmasked patch embeddings. Here we gather\n",
    "            # all the patches that should be unmasked.\n",
    "            unmasked_embeddings = tf.gather(\n",
    "                patch_embeddings, unmask_indices, axis=1, batch_dims=1\n",
    "            )  # (B, unmask_numbers, projection_dim)\n",
    "\n",
    "            # Get the unmasked and masked position embeddings. We will need them\n",
    "            # for the decoder.\n",
    "            unmasked_positions = tf.gather(\n",
    "                pos_embeddings, unmask_indices, axis=1, batch_dims=1\n",
    "            )  # (B, unmask_numbers, projection_dim)\n",
    "            masked_positions = tf.gather(\n",
    "                pos_embeddings, mask_indices, axis=1, batch_dims=1\n",
    "            )  # (B, mask_numbers, projection_dim)\n",
    "\n",
    "            # Repeat the mask token number of mask times.\n",
    "            # Mask tokens replace the masks of the image.\n",
    "            # mask_tokens: (T, ps*ps)\n",
    "            mask_tokens = tf.repeat(self.mask_token, repeats=self.num_mask, axis=1)\n",
    "            # mask_tokens = (mask_numbers, projection_dim) \n",
    "            mask_tokens = tf.repeat(\n",
    "                mask_tokens[tf.newaxis, ...], repeats=batch_size, axis=0\n",
    "            )\n",
    "            # Embed the tokens (GRU)\n",
    "            mask_tokens = tf.unstack(mask_tokens, axis=-2)\n",
    "            for i in range(len(mask_tokens)):\n",
    "                mask_tokens[i] = self.projection(mask_tokens[i])\n",
    "            # (B, num_patches, projection_dim)\n",
    "            mask_tokens = tf.stack(mask_tokens, axis=1)\n",
    "            # Get the masked embeddings for the tokens.\n",
    "            masked_embeddings = mask_tokens + masked_positions\n",
    "            return (\n",
    "                unmasked_embeddings,  # Input to the encoder.\n",
    "                masked_embeddings,  # First part of input to the decoder.\n",
    "                unmasked_positions,  # Added to the encoder outputs.\n",
    "                mask_indices,  # The indices that were masked.\n",
    "                unmask_indices,  # The indices that were unmaksed.\n",
    "            )\n",
    "\n",
    "    def get_random_indices(self, batch_size):\n",
    "        # Create random indices from a uniform distribution and then split\n",
    "        # it into mask and unmask indices.\n",
    "        rand_indices = tf.argsort(\n",
    "            tf.random.uniform(shape=(batch_size, self.num_patches)), axis=-1\n",
    "        )\n",
    "        mask_indices = rand_indices[:, : self.num_mask]\n",
    "        unmask_indices = rand_indices[:, self.num_mask :]\n",
    "        return mask_indices, unmask_indices\n",
    "\n",
    "    def generate_masked_image(self, patches, unmask_indices):\n",
    "        # Choose a random patch and it corresponding unmask index.\n",
    "        idx = np.random.choice(patches.shape[0])\n",
    "        patch = patches[idx]\n",
    "        unmask_index = unmask_indices[idx]\n",
    "\n",
    "        # Build a numpy array of same shape as patch.\n",
    "        new_patch = np.zeros_like(patch)\n",
    "\n",
    "        # Iterate of the new_patch and plug the unmasked patches.\n",
    "        for i in range(unmask_index.shape[0]):\n",
    "            new_patch[:, unmask_index[i], ...] = patch[:, unmask_index[i], ...]\n",
    "        return new_patch, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f19b9e52-11fa-4e43-9036-a886f6730b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, dropout_rate, hidden_units):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_encoder(\n",
    "    num_heads=ENC_NUM_HEADS,\n",
    "    num_layers=ENC_LAYERS,\n",
    "    projection_dim=ENC_PROJECTION_DIM,\n",
    "    transformer_units=ENC_TRANSFORMER_UNITS,\n",
    "    epsilon=LAYER_NORM_EPS,\n",
    "):\n",
    "    inputs = layers.Input(\n",
    "        (None, projection_dim)\n",
    "    )\n",
    "    x = inputs\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=epsilon)(x)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, x])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=epsilon)(x2)\n",
    "\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add()([x3, x2])\n",
    "\n",
    "    outputs = layers.LayerNormalization(epsilon=epsilon)(x)\n",
    "    return keras.Model(inputs, outputs, name=\"mae_encoder\")\n",
    "\n",
    "\n",
    "def create_decoder(\n",
    "    num_layers=DEC_LAYERS,\n",
    "    num_heads=DEC_NUM_HEADS,\n",
    "    num_patches=NUM_PATCHES,\n",
    "    enc_projection_dim=ENC_PROJECTION_DIM,\n",
    "    dec_projection_dim=DEC_PROJECTION_DIM,\n",
    "    epsilon=LAYER_NORM_EPS,\n",
    "    transformer_units=DEC_TRANSFORMER_UNITS,\n",
    "    image_size=IMAGE_SIZE\n",
    "):\n",
    "    inputs = layers.Input(\n",
    "        (num_patches, enc_projection_dim)\n",
    "    )\n",
    "    x = layers.Dense(dec_projection_dim)(inputs)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=epsilon)(x)\n",
    "\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=dec_projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, x])\n",
    "\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=epsilon)(x2)\n",
    "\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add()([x3, x2])\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=epsilon)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    pre_final = layers.Dense(units=image_size * image_size * 3, activation='sigmoid')(x) # tanh sigmoid\n",
    "    outputs = layers.Reshape((image_size, image_size, 3))(pre_final)\n",
    "\n",
    "    return keras.Model(inputs, outputs, name=\"mae_decoder\")\n",
    "\n",
    "class MaskedAutoencoder(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape=INPUT_SHAPE,\n",
    "        output_shape=OUTPUT_SHAPE,\n",
    "        crop_size=(CROP_SIZE, CROP_SIZE),\n",
    "        image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        patch_size=PATCH_SIZE,\n",
    "        num_patches=NUM_PATCHES,\n",
    "        time_len=TIME_LEN,\n",
    "        mask_proportion=MASK_PROPORTION,\n",
    "        enc_projection_dim=ENC_PROJECTION_DIM,\n",
    "        enc_transformer_units=ENC_TRANSFORMER_UNITS,\n",
    "        num_enc_heads=ENC_NUM_HEADS,\n",
    "        num_enc_layers=ENC_LAYERS,\n",
    "        num_dec_layers=DEC_LAYERS,\n",
    "        num_dec_heads=DEC_NUM_HEADS,\n",
    "        dec_projection_dim=DEC_PROJECTION_DIM,\n",
    "        dec_transformer_units=DEC_TRANSFORMER_UNITS,\n",
    "        epsilon=LAYER_NORM_EPS,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_augmentation_model = get_train_augmentation_model(\n",
    "            input_shape=input_shape,\n",
    "            crop_size=crop_size,\n",
    "            image_size=image_size,\n",
    "        )\n",
    "        self.test_augmentation_model = get_test_augmentation_model(\n",
    "            input_shape=input_shape,\n",
    "            image_size=image_size,\n",
    "        )\n",
    "        self.patch_layer = Patches(\n",
    "            patch_size=patch_size,\n",
    "            time_len=time_len,\n",
    "        )\n",
    "        self.patch_encoder = PatchEncoder(\n",
    "            patch_size=patch_size,\n",
    "            time_len=time_len,\n",
    "            projection_dim=enc_projection_dim,\n",
    "            mask_proportion=mask_proportion,\n",
    "            downstream=False\n",
    "        )\n",
    "        self.encoder = create_encoder(\n",
    "            num_heads=num_enc_heads,\n",
    "            num_layers=num_enc_layers,\n",
    "            projection_dim=enc_projection_dim,\n",
    "            transformer_units=enc_transformer_units,\n",
    "            epsilon=epsilon,\n",
    "        )\n",
    "        self.decoder = create_decoder(\n",
    "            num_layers=num_dec_layers,\n",
    "            num_heads=num_dec_heads,\n",
    "            num_patches=num_patches,\n",
    "            enc_projection_dim=enc_projection_dim,\n",
    "            dec_projection_dim=dec_projection_dim,\n",
    "            epsilon=epsilon,\n",
    "            transformer_units=dec_transformer_units,\n",
    "            image_size=image_size[0],\n",
    "        )\n",
    "        self.resize = layers.Reshape((-1, patch_size * patch_size * 3))\n",
    "        self.mse_loss = tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\")\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # Encode the patches.\n",
    "        (\n",
    "            unmasked_embeddings,\n",
    "            masked_embeddings,\n",
    "            unmasked_positions,\n",
    "            mask_indices,\n",
    "            unmask_indices,\n",
    "        ) = self.patch_encoder(inputs)\n",
    "\n",
    "        # Pass the unmaksed patche to the encoder.\n",
    "        encoder_outputs = self.encoder(unmasked_embeddings)\n",
    "\n",
    "        # Create the decoder inputs.\n",
    "        encoder_outputs = encoder_outputs + unmasked_positions\n",
    "        decoder_inputs = tf.concat([encoder_outputs, masked_embeddings], axis=1)\n",
    "\n",
    "        # Decode the inputs.\n",
    "        decoder_outputs = self.decoder(decoder_inputs)\n",
    "        return decoder_outputs\n",
    "\n",
    "    def train_step(self, data):\n",
    "        videos, next_frames = data\n",
    "        aug_videos, next_frame = self.train_augmentation_model([videos, next_frames])\n",
    "        # Patch the augmented images.\n",
    "        vid_patches, frame_patches = self.patch_layer([aug_videos, next_frame])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            decoder_outputs = self(vid_patches)\n",
    "            decoder_patches = tf.image.extract_patches(\n",
    "                images=decoder_outputs,\n",
    "                sizes=[1, self.patch_layer.patch_size, self.patch_layer.patch_size, 1],\n",
    "                strides=[1, self.patch_layer.patch_size, self.patch_layer.patch_size, 1],\n",
    "                rates=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "            )\n",
    "            # Calculate loss on all patches.\n",
    "            loss_output = self.resize(decoder_patches)\n",
    "            loss_patch = self.resize(frame_patches)\n",
    "            # Calculate loss on masked patches.\n",
    "            # loss_patch = tf.gather(\n",
    "            #     loss_patch,\n",
    "            #     mask_indices,\n",
    "            #     axis=1,\n",
    "            #     batch_dims=1\n",
    "            # )\n",
    "            # loss_output = tf.gather(\n",
    "            #     loss_output,\n",
    "            #     mask_indices,\n",
    "            #     axis=1,\n",
    "            #     batch_dims=1\n",
    "            # )\n",
    "            # Compute the total loss.\n",
    "            # Calculate loss on masked patches\n",
    "            # total_loss = self.compiled_loss(loss_patch, loss_output)\n",
    "            # # Calculate loss on all outputs\n",
    "            total_loss = self.mse_loss(frame_patches, decoder_patches)\n",
    "        # Apply gradients.\n",
    "        train_vars = [\n",
    "            self.train_augmentation_model.trainable_variables,\n",
    "            self.patch_layer.trainable_variables,\n",
    "            self.patch_encoder.trainable_variables,\n",
    "            self.encoder.trainable_variables,\n",
    "            self.decoder.trainable_variables,\n",
    "        ]\n",
    "        grads = tape.gradient(total_loss, train_vars)\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        tv_list = []\n",
    "        for (grad, var) in zip(grads, train_vars):\n",
    "            for g, v in zip(grad, var):\n",
    "                tv_list.append((g, v))\n",
    "        self.optimizer.apply_gradients(tv_list)\n",
    "\n",
    "        # Report progress.\n",
    "\n",
    "        self.loss_tracker.update_state(total_loss)\n",
    "        self.mae_metric.update_state(loss_patch, loss_output)\n",
    "        return {\"loss\": self.loss_tracker.result(), \"mae\": self.mae_metric.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        videos, next_frames = data\n",
    "        aug_videos, next_frame = self.test_augmentation_model([videos, next_frames])\n",
    "        vid_patches, frame_patches = self.patch_layer([aug_videos, next_frame])\n",
    "\n",
    "        decoder_outputs = self(vid_patches)\n",
    "        decoder_patches = tf.image.extract_patches(\n",
    "            images=decoder_outputs,\n",
    "            sizes=[1, self.patch_layer.patch_size, self.patch_layer.patch_size, 1],\n",
    "            strides=[1, self.patch_layer.patch_size, self.patch_layer.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        # Calculate loss on all patches.\n",
    "        loss_output = self.resize(decoder_patches)\n",
    "        loss_patch = self.resize(frame_patches)\n",
    "        # Calculate loss on masked patches.\n",
    "        # loss_patch = tf.gather(\n",
    "        #     loss_patch,\n",
    "        #     mask_indices,\n",
    "        #     axis=1,\n",
    "        #     batch_dims=1\n",
    "        # )\n",
    "        # loss_output = tf.gather(\n",
    "        #     loss_output,\n",
    "        #     mask_indices,\n",
    "        #     axis=1,\n",
    "        #     batch_dims=1\n",
    "        # )\n",
    "        # Compute the total loss.\n",
    "        # Calculate loss on masked patches\n",
    "        # total_loss = self.compiled_loss(loss_patch, loss_output)\n",
    "        # # Calculate loss on all outputs\n",
    "        total_loss = self.mse_loss(frame_patches, decoder_patches)\n",
    "        self.loss_tracker.update_state(total_loss)\n",
    "        self.mae_metric.update_state(loss_patch, loss_output)\n",
    "        return {\"loss\": self.loss_tracker.result(), \"mae\": self.mae_metric.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [self.loss_tracker, self.mae_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8b31e2d-6f6c-49b6-8966-14e36dc49b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 03:07:32.536756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 03:07:32.537593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 03:07:32.548769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 03:07:32.549593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 03:07:32.550880: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-19 03:07:32.854430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 03:07:32.855143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 03:07:32.855808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 03:07:32.856461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 03:07:33.626414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 03:07:33.627190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 03:07:33.627937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 03:07:33.628685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 03:07:33.629433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 03:07:33.630092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13823 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2022-05-19 03:07:33.630696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 03:07:33.631361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13823 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "mae_model = MaskedAutoencoder(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    output_shape=OUTPUT_SHAPE,\n",
    "    crop_size=(CROP_SIZE, CROP_SIZE),\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    patch_size=PATCH_SIZE,\n",
    "    num_patches=NUM_PATCHES,\n",
    "    time_len=TIME_LEN,\n",
    "    mask_proportion=MASK_PROPORTION,\n",
    "    enc_projection_dim=ENC_PROJECTION_DIM,\n",
    "    enc_transformer_units=ENC_TRANSFORMER_UNITS,\n",
    "    num_enc_heads=ENC_NUM_HEADS,\n",
    "    num_enc_layers=ENC_LAYERS,\n",
    "    num_dec_layers=DEC_LAYERS,\n",
    "    num_dec_heads=DEC_NUM_HEADS,\n",
    "    dec_projection_dim=DEC_PROJECTION_DIM,\n",
    "    dec_transformer_units=DEC_TRANSFORMER_UNITS,\n",
    "    epsilon=LAYER_NORM_EPS,\n",
    ")\n",
    "mae_model.load_weights('models/KTH_500/')\n",
    "\n",
    "# Extract the patchers.\n",
    "patch_layer = mae_model.patch_layer\n",
    "patch_encoder = mae_model.patch_encoder\n",
    "patch_encoder.downstream = True  # Swtich the downstream flag to True.\n",
    "\n",
    "# Extract the encoder.\n",
    "encoder = mae_model.encoder\n",
    "decoder = mae_model.decoder\n",
    "\n",
    "# Pack as a model.\n",
    "downstream_model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input((15, IMAGE_SIZE, IMAGE_SIZE, 3)),\n",
    "        patch_layer,\n",
    "        patch_encoder,\n",
    "        encoder,\n",
    "        decoder\n",
    "    ],\n",
    "    name=\"linear_probe_model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc7e82c2-d038-4fa8-a6bd-b394ee8c7f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augmentation_model = mae_model.train_augmentation_model\n",
    "test_augmentation_model = mae_model.test_augmentation_model\n",
    "\n",
    "\n",
    "files = tf.data.Dataset.list_files(\"datasets/KTH_tfrecords/training/*.tfrecord\")\n",
    "train_ds = files.interleave(\n",
    "    lambda x: tf.data.TFRecordDataset(x).prefetch(100),\n",
    "    cycle_length=8\n",
    ")\n",
    "train_ds = train_ds.map(parse, num_parallel_calls=AUTO)\n",
    "train_ds = train_ds.repeat()\n",
    "train_ds = (\n",
    "    train_ds\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(\n",
    "            lambda x, y: (test_augmentation_model(x), y), num_parallel_calls=AUTO\n",
    "        )\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "\n",
    "files = tf.data.Dataset.list_files(\"datasets/KTH_tfrecords/validation/*.tfrecord\")\n",
    "val_ds = files.interleave(\n",
    "    lambda x: tf.data.TFRecordDataset(x).prefetch(100),\n",
    "    cycle_length=8\n",
    ")\n",
    "val_ds = val_ds.map(parse, num_parallel_calls=AUTO)\n",
    "val_ds = val_ds.repeat()\n",
    "val_ds = (\n",
    "    val_ds\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(\n",
    "            lambda x, y: (test_augmentation_model(x) ,y), num_parallel_calls=AUTO\n",
    "        )\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a864e027-7bfe-40bc-b507-3102f1b4cfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos: (15, 48, 48, 3)\n",
      "Next frame: ()\n"
     ]
    }
   ],
   "source": [
    "videos, labels = next(iter(train_ds))\n",
    "print(f\"Videos: {videos[0].get_shape()}\")\n",
    "print(f\"Next frame: {labels[0].get_shape()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83df4b99-c2f7-444a-92f4-48eae872cac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"432\" height=\"288\" controls autoplay loop>\n",
       "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAHGZ0eXBNNFYgAAACAGlzb21pc28yYXZjMQAAAAhmcmVlAAA0XW1kYXQAAAKuBgX//6rcRem9\n",
       "5tlIt5Ys2CDZI+7veDI2NCAtIGNvcmUgMTU1IHIyOTE3IDBhODRkOTggLSBILjI2NC9NUEVHLTQg\n",
       "QVZDIGNvZGVjIC0gQ29weWxlZnQgMjAwMy0yMDE4IC0gaHR0cDovL3d3dy52aWRlb2xhbi5vcmcv\n",
       "eDI2NC5odG1sIC0gb3B0aW9uczogY2FiYWM9MSByZWY9MyBkZWJsb2NrPTE6MDowIGFuYWx5c2U9\n",
       "MHgzOjB4MTEzIG1lPWhleCBzdWJtZT03IHBzeT0xIHBzeV9yZD0xLjAwOjAuMDAgbWl4ZWRfcmVm\n",
       "PTEgbWVfcmFuZ2U9MTYgY2hyb21hX21lPTEgdHJlbGxpcz0xIDh4OGRjdD0xIGNxbT0wIGRlYWR6\n",
       "b25lPTIxLDExIGZhc3RfcHNraXA9MSBjaHJvbWFfcXBfb2Zmc2V0PS0yIHRocmVhZHM9OSBsb29r\n",
       "YWhlYWRfdGhyZWFkcz0xIHNsaWNlZF90aHJlYWRzPTAgbnI9MCBkZWNpbWF0ZT0xIGludGVybGFj\n",
       "ZWQ9MCBibHVyYXlfY29tcGF0PTAgY29uc3RyYWluZWRfaW50cmE9MCBiZnJhbWVzPTMgYl9weXJh\n",
       "bWlkPTIgYl9hZGFwdD0xIGJfYmlhcz0wIGRpcmVjdD0xIHdlaWdodGI9MSBvcGVuX2dvcD0wIHdl\n",
       "aWdodHA9MiBrZXlpbnQ9MjUwIGtleWludF9taW49MTAgc2NlbmVjdXQ9NDAgaW50cmFfcmVmcmVz\n",
       "aD0wIHJjX2xvb2thaGVhZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42MCBx\n",
       "cG1pbj0wIHFwbWF4PTY5IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAAA7IZYiE\n",
       "ABH//veIHzLLafk613IR560urR9Q7kZxXqS9/iAAAAMAFpyZQ/thx05aw0AAQomzf1k0Br4gCaJE\n",
       "1FPnsLcUBxu/cZpqR543x1uJ9JjCetFbGaC93z0Tj4rjWkRhTo72+HjQdDNz2erzv2alatP3T4yJ\n",
       "2MaBtM8osPdN+AJOi7heugVLVlqwWVc2kewz9gcCN5C11rME9XQh6npSOFCXZW3vuqruZe83cAP3\n",
       "XZZxEmZCPLaAchewer10dnM31u2oAy7CT+ui/wXkEC4HFpbjazrQAQAN4hsd9CFmpkyFc/vjvCkW\n",
       "DkrXOfO8cySZDVeHFfY2ZTgwVK+s25QPX7/TmST6zYPuLc05C73ajiBMSQwXbw+8o4nmsSMnGXCR\n",
       "/lFNXqsPKKk8vpllFMxiuNVw4TD/P2N3851kTkAobhG5wONhuLzomUqC2n8oTMjpUQqS+vTVVfZg\n",
       "BB296SXlNP1nLRlVs17GD4b4MMxDgllL+zNXVP+i3ghzCHeg78RM4wMGjwT2qsMyRZJTuzS7dK5z\n",
       "BGaL8dOxGfl0TN911SqYmc7z1iZ/Yc1L1Zazz/GJGHFiuFUSPtZK7/YeKVl6MCNssD4rZhNfdr/d\n",
       "mcFtQ9Dr93L57QPjXZFt9/nHpLn3J6AYlk9K9E+tLGnqZqz0Qmro8XKbOD6JoZ5TS4Y3KZnpbpJw\n",
       "FahMay2Hdhkk/gPefe3OHjqW8ytBYSp0cTBLxReqDlV0hOuvEUstEo7oAgHG4gTujIq5xs55ffK+\n",
       "kqND9XU50K8nreBI/W1rNmjlP2VTa3rQeF8eVg2j1FpFyP6yuopuUYyjKSPj7PgXGs7K4mLasc9X\n",
       "6pUg0sYeaqsnFiVKvb6aSeqGb5LTiZHB03lbYSZbm50F07lalTz6qjm/hcve+aPl3H2TnmZkTzi9\n",
       "lXdiSGC4E8nLTJIc4aXWWSYTNyxUDRmDq4LBbgHeDNeXmt1i2IkTvA9wdHETMQ77E5XVco+D/zLN\n",
       "k1tS36JdMCVo5Ta1aW+gZPAIEHW1nyZuqRCy4hmdea3bmzU1naao7nkxM/YHlU4UGF4S208TKxUO\n",
       "ybHhyn7kHPi2o+vrLxs3wG6OYE/FdNBWs/0Ii6D0y3odK0smBrb6l0NXkbbWqovpts1k6+N/XOfN\n",
       "EtSvc5OtqBMHOo7YrNOu4ChiWgkR+hAItvUwrpwV6tegfykg3O600Cy9Ks8OcNcdZTVRUV/qQkln\n",
       "wJibuZiCYIA+VEqjsUOb1AISCsTTS8QDsmjGTeOwLBNPOD+mRFs9Iu+UxlA3zS6ge82G/nJ3GOnv\n",
       "kebVJm24Dr9SPi678SCI3CIUvD1MeD7m193M3CUUZ1+7RJ7dzT7JwSKk0OQDRi3jBvpjAOw+c9GI\n",
       "1biwja7JZm/iVydmYPODaQiF6gpsY5oyaH80Q7E2LzMXNeR85Va/KfaMsDbKFcMa20Wpezs5BhAK\n",
       "EZiXJeQYguhLED3A+8vqrlDhVBvUZjAkVJndzHuvejbinkBYwaJYM0Mc8Gdu3ayQFgglN+FC6QOz\n",
       "xb1BjCjNTY1N0PQrvexAu0OTSc+0l5tmAqW767GXLmET/WD5ekJkowvfauMxCB5YO1T52B6q1X+N\n",
       "+uuxM1zMZsY8j5qoq+M2ymbcIDtiX12hiKiGmfmGoGuu6HzyoMZsmghghYJNspmYMpjUit18cnux\n",
       "t5HghC5LzovMAO9XcqFLOfehfvA34yTKoxeDzbRSz9QtvuErkzXAs/rsCUdMvA4+cjgO21z9VD+n\n",
       "/iXelcFJcJvXjBol155jHCy+EMEf0wFzU9SvCCR9TdB+hj4FU+4dog+fwJ6JYCEMDDPgVTp38KWI\n",
       "meRh+1pwM2xhbyD/mh+rzo38WQDweXP8spE0tebGGrqYfsjhaEtANByuRo3lsfIqaEZVS/T3/zij\n",
       "tyXXak3wv0HvJtuYNATRPnqWVYNC/ctuIP9QR0WtPyaZ3J96ADcbdnI+mneozqgpYMGv4gMSP3lS\n",
       "KCXikaEo7Rs9BoalBrTBo24IuRx3WcY8Em+5FBjauFtZB4XYAaHsvhqE1CNjyR6SPIxM9I4YY/Bb\n",
       "6IdMMF6X7UzlDcfg2YiNQ91Uvq/uAe3n57+ZGsWP16a3nnd28Ibmh3joaJhGOHtLhqE5dtW4UITE\n",
       "Xy3/C2ceqgkn9DCZKOvNDpWoRuY7jrLTBC1q7COj3pv2UpzFH0IdtAU1sTu94Dtb26IqmHHc1MBz\n",
       "jjm62RoSWpZW3P44srAELE7N1IvMYN+5mby/KA76JN9jPktxbYzgHCttC1pPMKiX4/1ve8CyI62z\n",
       "ggyBPCvFU6iu4aShAONpLVPKPETAM59Ebb58lMzxBfCnhnMfDPcG9Tm3RK91mlWtaNe+pjC9sCSL\n",
       "7syRabEAY7w3GvORTUa7dPp6qOQFh5/vcFYJjbaxCPhBmONLymU0Pd5soIgzHS07hm18IAEeU/SM\n",
       "oRTxS7IQwdbpNdpw6BNKDifN7T7kBAPwyLTPuqq82BUtO4SWhaEZO8+rzTWF7JLn311w7Q8zUJX9\n",
       "kOIDFhjgCfceRmtST0hOoI+WsIfSDbJbZdcnXTRgo8a3DoKPlP4pb+v6Q1qYMKyuoS8eaQJRj17R\n",
       "GrOxeUBdixFQ9Lo3Ts9BBRDZXh9YRZgukGPxdY+f28y0Z9pgIkcMHX1Ad0SbDtb+PnJyqzdY8LDP\n",
       "fvFO0A8bUiujwkZJjYExXcDxvKP0TuV2/xxZKrrROyxoDavaXqV2xE+bTgPRQDBVAtJVE6bwa3Rl\n",
       "sbZhUChxfiSCH9jktxXAGot3xicY4SsXWKCuCtzrO7t1W2MqNDs8nHKMA7OcyeYzQoBKtqwKZhQn\n",
       "cOEQSTLmBMU87mhdQNqJBNUPeZaNdziVmssszq6ZUP6PxGgGHQOvB+grCllne5RjX7j96IFL/khD\n",
       "eqTRp/djbDciA9XLzQhXn81804E29NvIMt++NOr/fMp1mLwoUCAZh645DVQZeODILWF6XxngVfnH\n",
       "cc2jZ6A9T8si/p2YCD76fEvHxP/6s+t/dL5RDlwzY7jHUx1o3Cl06yRVh1izDokUztdBimVf/rDX\n",
       "DZkgV4kFLuFvCj8mKoI1QMVh/VJ92+Zg2L+6Ng930sPvKHlOV4qkoiXJkVcpUMmhmz9krVSys13d\n",
       "f65MHEZcmQIb00veo3lMatJJN4xNUYkaPbO1Dl1KKJ7XIcQc2KeN/XachwiRz2IQuqpUSIMZ8/JE\n",
       "3XlnE/uPR4GVcvRqI9mf4UMyoOvc7brhMUlY0gs5uJ4vBo9LpkU95Pvv2ur2W4Lqlq6sZsXwq5I/\n",
       "og88xv+qW7niO/BhrUrhLaBq6aEAso93jM9vlO2E8JGlliQzc2lgLkVUG5K2rhTTL/bnqQMf9e/L\n",
       "NGH/IkBafRLe26A3j1oplTjyxKQlIyEGMP7uwtUSN0wTC9eNOfk1iNynZnL69JmdnaFlYgujGzgI\n",
       "b/M67V2tt43jUp7YezK9mvmo121Ez9BaszukV+TzbWOarmOC7okc9yyThKw/O9Bqi3ZBvxIySRCH\n",
       "X+h1cBMX24Okc5X7ygUEQViP99c9KA9lVA/N/0TV/CH7Kp57ZE7vgNxnrQAQLrNa8sQbbHkeXz/0\n",
       "idt0h5twRWhNbeOB9zBLE40jYDOMidYPdINS0tSstGUXvoZNcjfdPG6wuwWkdjklrKuajX01BCUf\n",
       "f45t0IkuWBEqqxLGmcdosI9IT/vJ/CTOs4fvVXuNADrNWSnHW2euSMUYkXEHR4Ckh9q2RONcvz5k\n",
       "BD/kPiDCxDVRdqtegHrgM+sNhbotnHNmgthxVo30/HRa75tWzWqgZ5vAv+p7eQo2gi1m0R5oKNH2\n",
       "SanVmQxsuZXppdr357vvxrFeCV6zjegRacVDRUn0l1ok4MVWJasXGydgO/v4e2hn7i2rXMEzVssQ\n",
       "ZPeGncoOASEgLlqpRieZtEstOQQqYMdU+gH13Sj+cZg0mvdbSQdBw+4sFS9Pe0TXqqp6FuOmzYHW\n",
       "B6RcAuiDBXxrlsgaSHMqGqiqRRtKjyeGuqvEpJ0H2ZtkC/pefP3gwxYA4pTH7onqBOV3+eq4SdBy\n",
       "9XCFS68Q9l5j2KCxr7UYh/xEJ2FEp7slimLlTdz/o0CWopyuGdp96ZkOyr95zI0KUomLZJN6jKpr\n",
       "SnrGcpCx+9D6oe8doo3JUBb3axURFLLAiueo+TNbpWvwT5TQ4Nz37T8az+eENLOWsclNPISfGI+F\n",
       "tabt9P/DzUG2d0UMYfi+Z9Okz+mjTK73+dPH5nFj0mn0lTDVaWQhkWiwWoPYMDtC+krQb4f+8pWw\n",
       "pQVP9Yyoo2B58qUI3Lp4IqS0Kd/Ng2xnDE6p3vTzb+DbxwhyXZIbPhchQfDvzfJoS6BMRxz92mus\n",
       "PtvqZCEPlIm/EjU1TBxcK7yl0Efgv3x3amdoTejPXG9rYN1+UumfmVXM9B+1L1kAXCs5IO7IoD2I\n",
       "LP4hr7TG2LRmfgyIqfkkjEKOoNPSqX9tA57y+gipHcvBGFR8Ie3NYhK+6zIhdn2hRf4ZwVocEgli\n",
       "Nmaj6Dpz667Izq2q0YhaulSLXQbjdJzZUWedvMNBxWkWiBe7Oo3/0ozHwbhncFaj/e/qSz68l2wF\n",
       "MZ9Ng4oY8RgDm+92moY+UVWpPUX4QtksDcpJ5N9G0nJoIZebstCKE5sjE+L46I7cadqrxgeBzZQ9\n",
       "gvobWPBOGj/I+NOq2zZhr480188Sl8Ltkpr/TcY9qhdVZvqG6vaikHhsBgTTtPR4ddnx6K7QkTTH\n",
       "KJ+LAww620qBwzlTsg2A2lss2X2wJ6yL56GPvu89tHeB13HrQrPhcsxDo+nhQ2KacozksAkH/VTP\n",
       "NyoFfrJSwUNrrnQOwbIGfIlbcATcGXPQsynHKkIsJuXBhZ+vQ8PzhhI565dhVWgt8Xor1IX3lm+7\n",
       "0WjXafqmdIEjzFwfOT9AGwVki6iuwZN0tRv9hS1uCGkinfDiQFxGYsBfiyWt2YXyYd2kictfVJ8M\n",
       "dSTKBN0QZb+2T1IHnygKikYuPGhgr8/xtFLnALURgdDz9vnLdWU8Lq/SnoxGERRWJFp2tFXskbOd\n",
       "u9IhOQ2jCKaCCVOVUnTcqgAEPQAABulBmiRsQR/+tSqBNtm5l7NhzuqX+YAhwNIAcOsOEQIf3U2v\n",
       "RVZvE3wy6AKeyEQm4F5ZVjia3tJYH+ydRMlFcPBlN2MD5QAUBC8zZqZb4BS+Dv20wT9NMfgt+k+k\n",
       "r2NQ9gZBG1IGvCx3HpikLldDAv+YhPOXJ/oIWsOOoz1ebUFdUHUIMQKd9fXpNR11GgkG2yD0BYKd\n",
       "cQ0OkCNu54YhvLG2RwVTYHJFlXkLj7xMFeNObrJtGygCxfFwGGRwn9e/0aOlhwtVt5KVAaH3nUp8\n",
       "OIeXDU27W4+XQfbSrfUUpYvOs34RGc/fPV7quhaJ2ziPqpBb9/bA5H6OHZth3H8B2NCXj17s0Ffe\n",
       "e1waq5DkqGBJfXPH3+ffw45QL3nAkDxDCkG3mgeeRWW8IJqnsiTyNxS2snX4kSjQ09ePnPMi4mpZ\n",
       "x9oqJZnz3hSWy69qf+g7CzuHDZMay9IMLu0YIy/NxVHCXMUFOwev1MBKibzxUQ7gUou4ib3bFL51\n",
       "UNQ2YmWe5TMtbezWFMPccYLaELufrCZhGG02PdVMEKQrIApj90jLORmNvA8iuQL8vyXeldwC1Jgj\n",
       "S97yK4Xy4jr7Ft2jO1wgHjLNNCpWgzuab6APINsFLlbMMKqQz+nibyk45jemRyF0B6Sunu5GBLZv\n",
       "tNitGPiINvS9Bh28tcu0Htv/WCf83dAspF1FQ9SmybDxV2P8PcFjeTx1FblmpObvxVChp+xQkxy7\n",
       "bYnDJKGzVAW3lTguceGw0/5oT4sJ+5IhB4feFOQf+4a0kEJQfTiApq474tesiSJ3lctx49LKTY5g\n",
       "H56KVQN4kbJF0TrVARVUfFyEDaUS1EDeISL/vcqM0XayZ7EF1U/IAi2YzlPUVnFO5m4lvS5E3zKV\n",
       "F5zesxtr5PhZ5UHUROkZDNjUBzP4bf3J0k4VZyG/dJJomocqoAgdTIG56ESlIY7y0ONjgjlFkZZS\n",
       "nKEgk6Ju94Hth2E9+F+vwpAWQCytuX8ZAIH3K8ZLtzQ9ro+/Zo4lwCCdHJZC6wH0MK1lbVf24PTw\n",
       "VKdTBkw7sp6VLvAuRgBhRSsRaJqKTAloDAiZJExICyrDykYMRlqcsK68wHMendni/5hzhhGauXTk\n",
       "v+fSYnxZU/o39XO9bAqrJOhcuhnO63FaANKgnW54kSuCxx/odeBCJL2UVu60nn3FS07euYcqjjmK\n",
       "DD27Y+8QPA1DoUCwysuHDGi6HI5613j5kpYIYwVH3VJHRqwyBbQ+xiVmINmLZyJ2U3q0unU5v2b1\n",
       "PuzOv1cwW7D8N3D5EapAgiNN20NUF8TYbH/X5cwJXJbOsTIGn8tGsWUc7MkJ8mnFEenXwXy3BHnM\n",
       "mq63OBl/vQNCiBXZQovq4cnKoarQ+Fy6RjIys2o5MAwnEUdYvQHlRIaemuATMoE4QlRJBuLeHUaP\n",
       "XfvXrI2vLvLL8u6wmHL8DhMplKTupr9UVgWFNNELvlN2jDlGqtD6Rl7/ntxzLJ3yT/7LgVDTvEW5\n",
       "XRt+wgwkckBHzek/o+xcmkky1nqxJ4yxlxNwoWs95iNiDSuobHpbNu7bHLEJfFC4MAq2wWaGL7v7\n",
       "KI76Un5vXAnvh+/qp7axfeNRSCL1Fk7Ow3vm32nihjVJz83IBVPvMm702zg4XcFZqdV0M/2gN2cF\n",
       "J+5GWnjXAlf86EjB+IUyXPFsejznYJv/HtT4LrConapH9Y3qrmarRi5RP5FvTdWDJCL+IMA5Aa0M\n",
       "hTU7RPGpCPfLe6/RKMRw483VGZewXxf44/nwte062mYv21fVEDa/alnwINZRbtvA5agpaiCX4pHu\n",
       "VR/AWRsqkC1wfYT2LoyiCEA6FJ3I129wcRwAhUgFfv8Dj02INlRaTijkoPcS8PQfnHLZVASNpAV2\n",
       "/W6Vwj+OCujob5A3GIixF5dR+uOUQTEEvBOncI1BqCHsTJRlgIYU5Sl8HEw2VSy1dO39qloGBMff\n",
       "63HcS1NeMFG5anLRukHI4lZoPtfNcPaehgywwdTHkkf7zMpYn1xTmtb8Pqwnud4270X73/nKIL+k\n",
       "gT3LrQhaP440BK1BcILG7JwMrgAhPYrx+qvJWNCmqhC4S4aXYvN3DxLr9C1PjQ+D8imVqAq5Tw4J\n",
       "k8yccJx5vsLnuPZ2BmZqQj9yJy/z4p8g5pxN0M3GXlwzWR1Ft65YS9z8XzaAw3g2vqQaIr959LMh\n",
       "Pk3FCy24fjRJ7PO/2JRy+FlkvRuZ4hLvinPqM9nGFL8YsllX8y2V7TPcD+wHOyHjrK8f1gzWP1qN\n",
       "1pvEovyGtYrx6+1fA18eDxPS2k413onG0scr94FyMShVTCEy6ngdKX3iVkehEoZnLMec5VB1R7vk\n",
       "TQoEZm2QeXWzznbyzrGT6iq/iMvU4g06RgAAAWBBnkJ4h38AVMXXlppEsABEbAJmq41skXv9SLGI\n",
       "BrNzA9hqLcP1ah82YTLPMRo6ENpkWrMUtb/on1Aw+HGXmphGGuVJDmzwzuLkbCvVy/fNNVBCOJrP\n",
       "78CFoY/za9DXz2CRSvgxgmYL5HqwN1XOGrl3sjW4+2OW7pFPBJve7MmVVPP383ap6sskKfrOdt7H\n",
       "3GOKzIQHofkdGlDeqE7bNYtO1D6Z7MLuahYwUd49Vpk/AawpU3ORjb2dRR3b75gJF1DMT/nbif8n\n",
       "fBrg97Rr4DCzTkiXgrZea4SdnU9j5W3hNY0KpxtDRjPl7JwaMmdOjJNq59E0XrVOSo2oflfSnXvr\n",
       "+0rBcJ9GBPxb5tpGuulBOwKA1PVROt8oF98L9Y6rZTf07dFvGXFwM/qSXP0oZ9WhZfno19QPooqD\n",
       "QqZirHV9fBkyZ9r7JJcoYfJaRFvJFYnTtUVSTFHelDqLtcOb8kQdAAAA1AGeYXRDfwAfXO00Sf4T\n",
       "8ABPMz2U5iYNh+/b909mfCZylVm6Z7ZuSKG4eRAbNiNLcsSp13mtBDxitL9mp6pr+arg7Wn0F7VC\n",
       "EzKIIewExTTlR9HjUX/swVUMWXr1jQeGXaHJjFK9DqKrEKM3DFUyL1lJmz6sTs8U4qYZq0VvWnWh\n",
       "pJUqYbk0Bggapp2qjJrkqKukKcEHOm0rjbZjjVgnvZxIivHP6pIy0MBSxcGAQS2JDzGlfW9jzJl9\n",
       "as0/8LbOpMothjkaPiDpp6qsu1J1+mDDPaHnAAAAkgGeY2pDfwA+EBS8dQCuaW6uAv+xc1krV7FD\n",
       "SX9/C7P5TizC8EfyNh7Me24fwy3rJW/UcfvMh8SqOGfo9ziab4Uo79nIFr0PWy0nvqxsCm+OzQlN\n",
       "jLQDZMfqCK1p0isCnGW4w3erJAkkh96H1B2WQVvdknErydwmG5hB68DWxtjvdydciSkRzFaT1QfQ\n",
       "U20p+W7BAAAHXEGaaEmoQWiZTAgh//6qVQAydu1Rz+qPwAC3Khi2Xuwy8+Jfr8tzJo7s8+trvw/m\n",
       "Ly8XDBcMKJvcKGPu1OR3A6F+paxOAw2NuUcZWVEyMKW/9e2gjjv36OcUTo21wnpqFV3F630ww8Bh\n",
       "HLI+ZaxGOQVCYq92IWwfLmvqJf0Rh1LtfW0oM12FUax9KxBhff9jShs7IVV0SjUkCWFXfgI6AQ+J\n",
       "qgNSUJmkWfb5bVUFxyrQw9sv0thbvl1yi+OzOTABJydtzTH8shCcEHA7QHLDq9pQNfIEjyW2oJiW\n",
       "EIDS69NWm8WEVpZcJhyOUTAjwbirRCa1vkk7rawO/EqI/UT3I+Pwj/6lkz9u3NJRaMt2+s0bM84C\n",
       "fd6s7++9TqTENdueBeJUubgfKcX/4pWSUK22mLexu+rSm7+kQ3GTAhKhy2gBQOd0RTf7WRl8DQwL\n",
       "C0xwlSTE1AefacCBnzmc44Zjsnwst6l3SicqtBSEBskoqARl1WReW7QNKlVqwB3Gwc9aKRiYo4B6\n",
       "pnVdVVDKI/e1XkeH3+NMJRWl15Fpm9u+WfjJqvXLPTgT00h2h6rOiXZmcWDE2Ca/WDPUxcf4XohH\n",
       "M6ufIVhD464fnMnIN6LbNkABdUFAg8lDPvq8seLrrjKrqu0+Q1xWkeL6uU/If96/fMUK+8YzLAPi\n",
       "QBO8p/75HHeyzVjcrolG2JdMvd2OsZTB38/jwd/renrE83fMFz/y5rr7bM3gSPGrk8O5QUCjC4CZ\n",
       "lKFktIOC9n0dBME66TLeaDmjTuFoEdU8wx4lMv3RT0ag6WAf7GMU1kUJMCZfk+kQxKn576hbAUNk\n",
       "Hm0pJ/18Ca9ClbE0LKYsiMPWe7hqbU3AKWCAQ+leYdRoImNsLDViba0iB8gCViUwzwd2WWh/rEf2\n",
       "mxW5oflczko8dRhM76dzus3cxQctQVQITxCYVGbLmbb9cluaAJFaRWuP4LzxfM6frWmO/CERbIKq\n",
       "Qd1POYYsfxgQBBnGbDfcmNZarGj1pePjmZ3ojZKDYv9fU274fbsyCWx4sm6g/5+4TQTLwDUnXcY2\n",
       "7pJaq04HkscrEZP31a5XrfDaanJSnsOalDEav5NTt55FgXoOKDoSFnR9JUrs6dMP1EnXdZvGE/Ef\n",
       "LIxJaHXqwHCALA/3sIzUiXExGy1masIXJ297rbj7USjj7R/mhFDtkQTnPuwhzqtGF5xXZjUdVcgz\n",
       "njbKPH4hAQWvvFhKdM25NhoA7ytX4rtBoebNT7m+2/0op4GF4UjHQe1PnG8nKClTSme1KJzLszCT\n",
       "UYGiQM8vv5x6+Z93VMB33pU2SYlrHSrsXLnBcABWoI1FJT30v463mOwm/KRB53Hh1UXCp37iEBzN\n",
       "u/sRla1o2zEDfrZwLcRCySGn1GvIdZewh9SJkRnf1YMBt+h60vV9GrLC+t7AwscKkkTL3NwnH+j1\n",
       "ISOQVLthp7BGecDzjHpSic9dZtHYRC9g+pttG0Ce6mpQNgszn2lcqvGHV2ElKwJZgb3ydUkQfhrf\n",
       "wh8K6h3uQQiIHkoKoZYBtTTcqoMJ/gWa31Evd058Ydem87MYt8JFuZ7RellJFQLVGzLx30XLOGFF\n",
       "qoQkO1u8F7aFHdN9v4frhOHGm3L9MqyBta7pK3r4zGGb4nvOZ2d6IGwGEgBLqgzxcenkOHcMLj8A\n",
       "/N1KVbeE/KaAaTLkb9TX1iVOqNCIdGH7RTwjYRqvN8XIwkWCJtQ1Mz/jJX2WoB5Ra4DUGzEwjgwk\n",
       "dxxVPxx/xIWhIoELudWYODQLBKtlwFC4kRCsG1RxJLY139YXSyGzUTKL+RD+IepmYiTCnKqB+zuU\n",
       "Xwyx6Eiaz0HpGogXcu/7BhISv3QbxQ0EN7qTw+oO4cCqLRNqhquBdobSifdw9CyRE4cd1ALmLJXi\n",
       "GiV3ZN4vYSmMkrbZ5g5EQCzm7DbVqL2w+lQi86RJS01gxzwhU/6oKP6sBcvODjLGmu7qVFjO0jay\n",
       "5UaqMNJNs/wpIysEnXaL3nSnOpGR73B1vWh+k5C76qQph5+etsNzR7aAUq6Sa9R9TYl2NMgHWXOq\n",
       "Vrj6DQ/wZQB3IJNH4Tbrvb10Gm09xNyvwJwcmneQfvBraSg8Fd8oOZFZSFtE6xMOukoBxDNwb2JC\n",
       "oEGMVRSObFL4HahhRpegEBztEwgcRMQir3PAh6Rho3kpwCcLeZp5IV+iQ2gOGaQN4Si5dz7g3ONo\n",
       "Bw3K/awxSuSJBhQSjunUFlX5R2UT7UeSsj+DqTiv/ChyDcbK0Yj2XMDnAkkgrjGI/O+brPXlZRFJ\n",
       "Hz3a4KnJyC9J4GeZY4fxxDq3YLex4mjdaqJ9yVwup29Ex46tg7m7Z2rvSQMDTop0OXskNy+bCE5m\n",
       "M4e7C12Otu+TqJE5zltcb/Khaf2RkMHRczXaL+0abpgJDSu0KhzoanWo7+IZ+xSYzVxFnmNZFWAL\n",
       "IzvBtIMnA6LRqzxTsaDY/all9jaFdWfi7xee6uD8JkojQK5k8B0nE9CCxpgkghfirTCRjoix/E9E\n",
       "KYUX+ju1Zd6lD8gsywAAAPNBnoZFESw7/wAu5infagMMkvlZf0NtJXs9WfhPCzlUIeMbPGhJPN4K\n",
       "3PqB4i6UrFY15UJ/y7zunpsSuB0T//fx2wnUB6jF+ZrCz2w4ulYP58URIfK31G8bPLSk4t645SB+\n",
       "KK1ndLBxkKXuvVnLEohI74eldseTVCTnBXa7LOLoMSgjUnrYJ4ttoaiElDx8h3BJRcFty1D6nYMq\n",
       "CTf4v8xwvPWBIu2mOFiWqUsmdAyy2GBgRQ++odcJKB4tyQNj90fC38482zy3SXBvXis28wzuUr+4\n",
       "NphX3bPsRl8NF0r3jVoizv4ztsnNZ5014uffuwkfLIEAAAB9AZ6ldEN/AEVvLBHf5Lo5oAVnBTpT\n",
       "VCXalWJkR8SBWWrl116/GZ7a0euLyR9T7vVkMYfv11cdID754Zd5BtpfY/davZH1PwK5g0wmz+zv\n",
       "8YEqbzLiPsQspikkFEJRK62anB7/JeWmJzhi0TVO/8kD1Xz9vK6v0M9wO0RJGz8AAAB7AZ6nakN/\n",
       "ACCxpurDVR99wAFratTPODYw5Irs/YZxZ+Ff2HM7MinvtY/wbafw+fLylTDXKrIBbUE0WZTogiAb\n",
       "MulXO3MEhCExMAbozWzy5ZJwXWLqrFbc7ex70kj1RbaGTdShz+cTYA6sxk3/7diOS3F8F8GC/UPD\n",
       "stXpAAAEO0GarEmoQWyZTAh///6plgCwm+MQAsfFYqkvrin+mQoXRoPVUvskACbEs7pvGLXD1RhR\n",
       "EnM1jOWXfWVRh3iEvPiae7NttGQXP7iS2TmMl5bNTZ0jTBRps3U8t6jfdQ5cIWWQla7g8q3q9Iwp\n",
       "eWH86xEsiOZWPBwZyYlwyPrPQl/5miPan/5j8QBdr0H9nuRJtcj8nYiPfyrvWpeN/c3alNoixGE8\n",
       "KWLBj5WzW3je6l/hUIXijPItAhHzjxJxzii1PM48IYSnBqk2ywgm1asqLcgC/2sRd9jWmjGZZTRZ\n",
       "BnxwOg/dBLxjfaQ36S1u/IJuvGgFCm8+I+ZRos2UnXTbS+i3xcyeFxn3lbUe5cBAHYyYTHHryHxr\n",
       "lqJPr8hdbv+hoeX3Bb+5ibskzHMukRzZAmEP7YjtoIIQRcjjoi7BH2OIxq/vVK9tLnn6O5/7mWiZ\n",
       "5B0423gfUt3kOr73YuekuACr0AMPquzJLO80WbL//8q1jpV5/DaYKs403QE3DI04hf4vGGyD6o49\n",
       "oHVSKMSILo4Og3nfl9bClqMdLX9bfkjFSXzFDIxoKH2oTxdRRkzpBAJzxKhw4IZlQV5pqiewKwaW\n",
       "k/Z9bzXzyW0ZWyUp3QuCKgcM9RduenTmC1i7bwwPmQhyNXK1QnVxc4FwaZPyVV56B8L2irbp/Hz6\n",
       "mPcAxmyYH5nx7fjUgZ9sOOUQdH8ZZfER9vSerRv97DyGYm6KEmU1++vK0Dv0UiaCEQC3Oy7AOlvw\n",
       "jyAffjh+sLqF75gLqcdA+hvaZGCJ+HHJSaAB+OFTt7tQP9yigiD21Ti7G16mKXoxTumfwE/8j6An\n",
       "swdatEqZy28lG0Ivb4f5hKJnhKMYtoJmmZXg9VdK6X6+nr8nwVCMSKUN0JGqfxWehvWEe1OzbB47\n",
       "fYEBov2sOZf1JR7LZvrFwZboLxdiSCpKScF+rFuOs+5jfjouW8E0lfBCDSupMkhr9VUnm1fwoblV\n",
       "GltWlyMb/ZorE2dIJT7OfO2uvHK+XcBeO04/rYvIb4xMGY916dn+w/BhzQrUASOnFnEaiu9x4zHT\n",
       "AfBwaqKNa6NYsArbZ+LwKXSFOlvm8otBiXrIwAxbJDu9THY2H2qugdHfnECG9s/lb63iTeUQmoxx\n",
       "xkH9mAZ0dDXsEUIOvm8QZogtXBkamV3/mR2Lr9Ok6krorh6lPwSyyv9X519FhQ9rLPNbO3xEp+nA\n",
       "vs5Dtzw0FVPZ+Y/CibDdM7g1Fc4NwmBM2a2EIHOEaM43j2TMS+xXVs2gayGhxx6b+kb2G2J8FK4k\n",
       "417VYQCWHRjwfrYfgvxjx9hlcvh0kS2zTlv5TOjma7CEIDfM8PBCn7ABqzZNmM/R781BUSJuGAM9\n",
       "pqD65R0uuysc4/6ZFIdt5HmGryelwdWiKbqn1BcQv5EGyKczzezxNDiEYMKpOZZI6nJEZ7IXPugs\n",
       "oOddPO3poAAAASVBnspFFSw7/wAlDub4AJjiATNAiUyvoHNuEbwJ9ektWepLootoRnO1syUoD1En\n",
       "9vToO2Z6dgFWRGwq56Attr/aC5RIcpgJWIYLQehznBgmbr/3wBS8ZniTR6+AjqjDzP/1l+Vioq8+\n",
       "kjEgiuhqc47Y8WufIqPoQpMRhwN6p0jZdv3EJK9sHuZFc7t5tOwQ0oikn7ysBFLQT2IvjOjqTM8j\n",
       "chaF+Wo/eAotEdjT7YuBR4v7dClpCqapF7ZfkyUa7+ICv8IeKtiCIUgFAjsQV/sKKEWnqWJJWbHE\n",
       "qZ/3G+sdl2BYnDNbXjgbDEkoHBO4E94nH65L3kYz5e7BlXUFbyv2zMXA6psiU1pKKAJufysyespH\n",
       "NU67sE9/Kg2M2o+0pee36NMhEQAAAM8Bnul0Q38APhqAoiDs6Qjxj2AA4KblSctqXYFdtVHIUgSP\n",
       "XbKU0KrDWQmZ1CSH/EETzRfaj0km0rwvoZlbZOrn69u841vd1e3CKQlkOAMs7pIgMfq51sCDMdgm\n",
       "LhRlejaN+I0rVtVz4f+HaoJPnmQCehS1SEbK4kSwEA1fy7KXrIhjxO0kMk0LLlAvJH9hYpPh7JOc\n",
       "t3+pLlr0/i+Yfvpc3VJoqxBkqrkY57+33b+B08067eiLbBQI+iIca0LTsY9/GEcv+uMRWZQ2TcUr\n",
       "TaoAAAD/AZ7rakN/ADtdpQwbMBjmomULeAG3cGgeYcjjWYc81aTwgDejBphFhR3UfjoSWoGHdJgt\n",
       "PHDb87rf5aekhglX459E3RR5h4JEul7NrknkeDw2i9hI33roiV93wlywayGYwFfU4nZPf+ilbE3z\n",
       "Hz2v/22PnjtiimBgqHHXowaLpaahQmhLjHPgFTEK4DuKVKOa9q/4lbaj3B1HzbWy6ClVfvr+rI8B\n",
       "r8e7zfIBDvq0py38bL6U/ktECoxkMybkiHkHqJ1TPe42RIcFlijL/cf0wjPlsORd+KoQlseu6c1S\n",
       "oIILrYx/BWeIH0Xm92Cc5/BcDZAbJcLeY+J25KhsVNSAAAAFikGa70uoQhBbIbBsBKQbASACG//+\n",
       "p4QBrow0AAJRQOnPwealbaxbfmJ8WYOLvxDjYtS5PZGoWL3DilN2+NyVXdkLTQpYk4tBz6XIq/hh\n",
       "4MI92BcgycCPWirlSNVzSgOzcNx8TJ9JoMbKKKgsMhcBS3b2RdUW/F32AnyhH/g5YEHlhJSFX38R\n",
       "u3FYtONkcjSGh9m58rR/L2L65lKWDvT59TsBRqu9fwDYZvRFDcnGNgOCG31cqbMRkdlfJ5nGR2zG\n",
       "sUNU26FOXuZKPLs0mlguKYxbeX8qALHaZabYge8iMfRJlwbqoY8Jo0T6megjPCwL4CtNU4fdGEzK\n",
       "QsGsh23jX/xNskxgxj3ZKRhQ7n19rFtStlLcAJKgVPwBOYIbD/WOypJSu37T5nL2TF1m09JMqC6Q\n",
       "2sceGIDP0yXvbQPED2VFEZAfUqTHTuIhtW+fYYD1UUEHRb9yUe8lnPU6c4gJ+7uAu9+Bdg7EuIob\n",
       "Tr1AerhifLG5DIdxRCfVdE7459IPIAspBqZLmmVpRPCwMR+2AMk4YMIv7fBqy5ALZmVTxO8INwBI\n",
       "1cCR3zMo6Ry23c+FRq+WODBpSpTz52WI+bXXezuJJKu+WmY8kdFj+01CJZwun9w/OGDljzOxPEqn\n",
       "xCeIWiACcBGcYonw+eYwYNABpJ3VPWY1aGuFXnizdsXeQga9s+DU3oLzOS28zjciub3f9EWA3KxX\n",
       "s2G8Bg072wVG9cB0YTr2k7BpI3utKNvFMe25WjtWrGbbQ9ggMdsrB5szE/i1wKm47UEbk0kugYg+\n",
       "uzZt+oF5JTb04B1c/EuY/nrIAaKx9I+j5Q3MMZRu6zujKRUjW6oAMG13yKVmnlD1sgj/QQ9knrmJ\n",
       "XqSy6GOUyTGou1vAnD4wnFCWeZQF3gnH5KxpXjqNxNyEOzd11uUTKJep1LB6NzP/Nt9BJF+k5ZrS\n",
       "w+eFjLELZ8nKNEF9LBICfvBEViOnHNOmqHTXjFLA1EiFazfUV8TkoDb3SoQtPBWOBaD2wI+03SfG\n",
       "9mit63P6+EbXkL105zuY0sWE/Bq5vePgYCsfh966b2r3wqGvTVd/PhRtz9kexHLcXhERDav9U5uJ\n",
       "CLN+g0vVFHfMCmeVYdynqLOzgerTFKM1N5C9Fepx6liYObuweXVwCU5HG2delX9lXa3gCpxtYUG8\n",
       "TBZoeow3wV0Hm33+ZIABhHGyz41S6tae0veBroNvBqTg/BDhK6tDp/Sx+ynMKkChvKcRRfbj1MEx\n",
       "PTtl/6KUNgcnB4oUd/U+8OvcR7gV9WZdPphLgMtkRJ63NdI+kBX7x0CEJYpXR0JabJoQhb0hqcBu\n",
       "rvF7Wm+Q0tLRfToq82adEkmGdDOsJ+N54ouzqY6L5xc0tY1UaOOtWPCKNdIRtPVYCK1qzvrbEfzr\n",
       "pqXOrvF+mIDTIO/TKMaWWJKYMbA7wbUSn3Hh1wdQU21Wcc8HeUJQY6wv4HB1xuEWPlb6Y+EiMYlp\n",
       "u5Wxh5UMMiw43EzPjcSh+SzAf5uOzchXUgP9K03Q9sAXxjYt5UnY9mZpct2H1aFtQSq/Vwnr0pR3\n",
       "BVlKdH33rnxQ86psqarBJGWjnX/pKew6i/YOGd+eHN7sdZRRSAw3rHCHzEEAI5Is4Ul6WzgBa9Qs\n",
       "AJse18RmWCdBzhNVEeeVyod/CSI8iNtgZjwTCvN9vqxjwC8hqzPVqKJM+G64ueAI202NtdgaqCeB\n",
       "n2cT2z2K4wnTVsy+V+V5RUJOWOAHinhJte55OXqb/OnlYtaqskEhfzE9QFTDdelQXIsBPmIYeUGu\n",
       "h27yryE7i6Y3PyO7c2Vv3ii23iScJIw5ThFTFgND35qh07LuJ36evY8HdYZooBt3IoSUV4D7zwnH\n",
       "2PZqDRm7OCwQ6X9Z7Yihh119hhD7b6TYAO5ShV+ZAAAB5UGfDUUVLDf/ANFkTbYx1ACyIXpgQiHR\n",
       "gh+eeHlXvmXC0Vv9OIZycrceeHC6B1Cy1j39o6TBoo3e7E9BcfUntrX3R1ZjDs4bSfZTk1oBKCWk\n",
       "erhykckInrZsl6j88i77EnVCVCmDQcJZLHJp+z4rA+upoZrvWaDTqMX/CNnxGc4brHX9bvMXWaPq\n",
       "SIM4KHiPnyOqQQyyTHAlVfYEWN4WiWbNGavhk0CBxuqIe1/E7wbVT4QXpTM0nZxj2SkjN23qi2uH\n",
       "wPYKUu1MU/K+6AVyL1wzXpMg3K2SwrzyHvCr48oGWHCB34Oee4Vuw7QlA9ZtHIyc1LOZvQBnK4Bt\n",
       "B8LsFovDlefQdy4ZVz3BIJF9XksauUzGvI/UsfopEULQPPfJbzgBUiHpZtOmDTpaAuQagMHwbY8Z\n",
       "92cvMtFwZ8+rajUaj0ohq3cfqT6U7qkA2U4G4jPucIuCsHaYoC+YmcYsgT/gSUGnPn9nI+bES9ra\n",
       "WYq1txCY0aLxTLyH1PpgP02B3FavgZ/cWzxzqZ9/Yn2vxXaDnAduxgXxDvij0AFTkUfWL6Vc3jk6\n",
       "ZisNKr40yQXmheZ0PERiD6CXwrP/gTlYmzJXkRx4eMLqHYZqOM2qE9hWcohLl8TtTqlLHZZkaWkj\n",
       "/oOid5SZAAABCAGfLmpDfwFXXSZdiO+pnvNL3+8NNIzpZPTAzJP7DFuWGRACyDJFke3OvWDNQnuV\n",
       "P8YqJsX8jAcOMaW/DuN1iPW8EcwougLItLdf0FjVpHcimcL+0S4iBB5Y/Bcr5XoCJLx3oIode0Xa\n",
       "CrBTVJj+s226hofaGimh6836u1YaPIQib8XLt5JHqozUYfiyNvD6Jq8rrlcnJaJCDhLWADJVocUu\n",
       "PDcKYPlWdDaZIGlaIXPSffPhFRYI6x1b04b1hRQs8+y7mf7Pe7tZkwObIldZiSlN6GwAfBdGMDuH\n",
       "GOS846XjeHfK+YzG8hSNYXTcNlJRDhQMeta0Paw00utZgu1wAp2iM3lZyHHNKQAAA+Ztb292AAAA\n",
       "bG12aGQAAAAAAAAAAAAAAAAAAAPoAAAGQAABAAABAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAA\n",
       "AAEAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAADEHRyYWsA\n",
       "AABcdGtoZAAAAAMAAAAAAAAAAAAAAAEAAAAAAAAGQAAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAA\n",
       "AAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAABsAAAASAAAAAAACRlZHRzAAAAHGVsc3QAAAAAAAAA\n",
       "AQAABkAAAAgAAAEAAAAAAohtZGlhAAAAIG1kaGQAAAAAAAAAAAAAAAAAACgAAABAAFXEAAAAAAAt\n",
       "aGRscgAAAAAAAAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVvSGFuZGxlcgAAAAIzbWluZgAAABR2bWhk\n",
       "AAAAAQAAAAAAAAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAAAAABAAAADHVybCAAAAABAAAB83N0YmwA\n",
       "AACzc3RzZAAAAAAAAAABAAAAo2F2YzEAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAABsAEgAEgAAABI\n",
       "AAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY//8AAAAxYXZjQwFkABX/\n",
       "4QAYZ2QAFazZQbCWhAAAAwAEAAADAFA8WLZYAQAGaOvjyyLAAAAAHHV1aWRraEDyXyRPxbo5pRvP\n",
       "AyPzAAAAAAAAABhzdHRzAAAAAAAAAAEAAAAQAAAEAAAAABRzdHNzAAAAAAAAAAEAAAABAAAAiGN0\n",
       "dHMAAAAAAAAADwAAAAEAAAgAAAAAAQAAFAAAAAABAAAIAAAAAAEAAAAAAAAAAQAABAAAAAABAAAU\n",
       "AAAAAAEAAAgAAAAAAQAAAAAAAAABAAAEAAAAAAEAABQAAAAAAQAACAAAAAABAAAAAAAAAAEAAAQA\n",
       "AAAAAQAAEAAAAAACAAAEAAAAABxzdHNjAAAAAAAAAAEAAAABAAAAEAAAAAEAAABUc3RzegAAAAAA\n",
       "AAAAAAAAEAAAEX4AAAbtAAABZAAAANgAAACWAAAHYAAAAPcAAACBAAAAfwAABD8AAAEpAAAA0wAA\n",
       "AQMAAAWOAAAB6QAAAQwAAAAUc3RjbwAAAAAAAAABAAAALAAAAGJ1ZHRhAAAAWm1ldGEAAAAAAAAA\n",
       "IWhkbHIAAAAAAAAAAG1kaXJhcHBsAAAAAAAAAAAAAAAALWlsc3QAAAAlqXRvbwAAAB1kYXRhAAAA\n",
       "AQAAAABMYXZmNTguMjAuMTAw\n",
       "\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = videos[0]\n",
    "final_video = video.numpy()\n",
    "\n",
    "for i in range(15):\n",
    "    frame = downstream_model.predict(tf.expand_dims(video, axis=0))\n",
    "    final_video = np.concatenate([final_video, frame], axis=0)\n",
    "    frame = tf.convert_to_tensor(frame)\n",
    "    video = tf.concat([video, frame], 0)\n",
    "    video = video[1:, ...]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "big_im = ax.imshow(final_video[0, :, :], cmap='gray')\n",
    "ax.set_axis_off()\n",
    "plt.close()\n",
    "\n",
    "def init():\n",
    "    big_im.set_data(final_video[0,:,:])\n",
    "def animate(j):\n",
    "    big_im.set_data(final_video[j,:,:])\n",
    "anim = animation.FuncAnimation(\n",
    "    fig,\n",
    "    animate,\n",
    "    init_func=init,\n",
    "    frames=16,\n",
    "    interval=100\n",
    ")\n",
    "HTML(anim.to_html5_video())"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
